"""
URLè½¬Markdownè½¬æ¢å™¨
æ•´åˆå„ä¸ªæ¨¡å—æä¾›å®Œæ•´çš„è½¬æ¢åŠŸèƒ½
"""

import asyncio
import logging
from pathlib import Path
from typing import Optional, List, Dict
from datetime import datetime
import re

from .web_downloader import WebDownloader
from .html_converter import HTMLConverter
from .media_downloader import MediaDownloader
from .clipboard_manager import ClipboardManager
from .special_site_handler import SpecialSiteHandler

logger = logging.getLogger(__name__)


class URLToMarkdownConverter:
    """URLè½¬Markdownè½¬æ¢å™¨"""

    def __init__(self, output_dir: str = "output", converter_lib: str = "auto"):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨

        :param output_dir: è¾“å‡ºç›®å½•
        :param converter_lib: è½¬æ¢åº“é€‰æ‹© ("markdownify", "html2text", "auto")
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.web_downloader = WebDownloader()
        self.html_converter = HTMLConverter(converter_lib)
        self.media_downloader = MediaDownloader(self.output_dir)
        self.clipboard_manager = ClipboardManager()
        self.special_site_handler = SpecialSiteHandler()

    def generate_filename(self, title: str, url: str) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å

        :param title: ç½‘é¡µæ ‡é¢˜
        :param url: ç½‘é¡µURL
        :return: å®‰å…¨çš„æ–‡ä»¶å
        """
        # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
        filename = re.sub(r'[^\w\s\-_.]', '', title)
        filename = re.sub(r'[-\s]+', '-', filename)
        filename = filename.strip('-')

        # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨URLçš„åŸŸå
        if not filename or len(filename) < 3:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            filename = parsed_url.netloc.replace('.', '-')

        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 50:
            filename = filename[:50]

        base_filename = filename
        filename = f"{filename}.md"

        return filename

    def get_unique_filename(self, filename: str) -> str:
        """
        è·å–å”¯ä¸€çš„æ–‡ä»¶åï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è‡ªåŠ¨æ·»åŠ æ•°å­—åç¼€

        :param filename: åŸå§‹æ–‡ä»¶å
        :return: å”¯ä¸€çš„æ–‡ä»¶å
        """
        output_path = self.output_dir / filename

        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›åŸæ–‡ä»¶å
        if not output_path.exists():
            return filename

        # æ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
        base_name = filename.rsplit('.', 1)[0]  # å»æ‰æ‰©å±•å
        extension = filename.rsplit('.', 1)[-1]  # è·å–æ‰©å±•å

        counter = 1
        while True:
            new_filename = f"{base_name}_{counter}.{extension}"
            new_path = self.output_dir / new_filename

            if not new_path.exists():
                return new_filename

            counter += 1

            # é˜²æ­¢æ— é™å¾ªç¯ï¼Œæœ€å¤šå°è¯•1000æ¬¡
            if counter > 1000:
                # å¦‚æœè¿˜æ˜¯é‡å¤ï¼Œæ·»åŠ æ—¶é—´æˆ³
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                return f"{base_name}_{timestamp}.{extension}"

    def build_markdown_document(self, page_info: Dict[str, str], url: str, markdown_content: str) -> str:
        """
        æ„å»ºå®Œæ•´çš„Markdownæ–‡æ¡£

        :param page_info: é¡µé¢ä¿¡æ¯
        :param url: åŸå§‹URL
        :param markdown_content: è½¬æ¢åçš„å†…å®¹
        :return: å®Œæ•´çš„Markdownæ–‡æ¡£
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºæ–‡æ¡£å¤´éƒ¨
        header_parts = [f"# {page_info['title']}", ""]

        # æ·»åŠ å…ƒæ•°æ®
        metadata = [
            f"**åŸå§‹é“¾æ¥ï¼š** [{url}]({url})",
            f"**ä¿å­˜æ—¶é—´ï¼š** {current_time}",
        ]

        if page_info['description']:
            metadata.append(f"**æè¿°ï¼š** {page_info['description']}")

        if page_info['author']:
            metadata.append(f"**ä½œè€…ï¼š** {page_info['author']}")

        if page_info['publish_time']:
            metadata.append(f"**å‘å¸ƒæ—¶é—´ï¼š** {page_info['publish_time']}")

        header_parts.extend(metadata)
        header_parts.extend(["", "---", ""])

        # ç»„åˆå®Œæ•´æ–‡æ¡£
        full_document = '\n'.join(header_parts) + '\n' + markdown_content

        return full_document

    async def convert_url_to_markdown(self, url: str, output_filename: str = None, download_media: bool = True) -> Optional[Path]:
        """
        å°†URLè½¬æ¢ä¸ºMarkdownæ–‡ä»¶

        :param url: è¦è½¬æ¢çš„URL
        :param output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
        :return: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†
            if self.special_site_handler.can_handle(url):
                logger.info(f"ä½¿ç”¨ç‰¹æ®Šå¤„ç†å™¨å¤„ç†: {url}")
                special_result = self.special_site_handler.get_content(url)

                if special_result:
                    html_content = special_result['html_content']
                    page_info = {
                        'title': special_result['title'],
                        'description': special_result['description'],
                        'author': special_result['author'],
                        'publish_time': special_result['publish_time']
                    }
                else:
                    logger.warning("ç‰¹æ®Šå¤„ç†å™¨å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ–¹å¼")
                    html_content = self.web_downloader.fetch_webpage(url)
                    if not html_content:
                        return None
                    page_info = self.web_downloader.extract_page_info(html_content)
            else:
                # è·å–ç½‘é¡µå†…å®¹
                html_content = self.web_downloader.fetch_webpage(url)
                if not html_content:
                    return None

                # æå–é¡µé¢ä¿¡æ¯
                page_info = self.web_downloader.extract_page_info(html_content)

                # æ£€æŸ¥æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢
                if self.web_downloader._is_js_redirect_page(html_content):
                    logger.warning(f"æ£€æµ‹åˆ°JavaScripté‡å®šå‘é¡µé¢ï¼Œå»ºè®®å®‰è£…seleniumå¤„ç†: {url}")
                    logger.info(self.special_site_handler.get_installation_guide())

            # logger.info(f"é¡µé¢æ ‡é¢˜: {page_info['title']}")

            # ä¸‹è½½åª’ä½“æ–‡ä»¶
            media_mapping = {}
            if download_media:
                # logger.info("å¼€å§‹ä¸‹è½½åª’ä½“æ–‡ä»¶...")
                media_urls = self.html_converter.extract_media_urls(html_content, url)
                # logger.info(f"å‘ç° {len(media_urls)} ä¸ªåª’ä½“æ–‡ä»¶")

                if media_urls:
                    media_mapping = await self.media_downloader.download_media_batch(media_urls, url)
                    # logger.info(f"æˆåŠŸä¸‹è½½ {len(media_mapping)} ä¸ªåª’ä½“æ–‡ä»¶")
                # else:
                #     logger.warning("æœªå‘ç°ä»»ä½•åª’ä½“æ–‡ä»¶URL")

            # æ¸…ç†HTMLå†…å®¹
            clean_html = self.html_converter.clean_html_for_conversion(html_content)

            # è½¬æ¢ä¸ºMarkdown
            markdown_content = self.html_converter.convert_html_to_markdown(clean_html)

            # æ›´æ–°åª’ä½“é“¾æ¥
            if media_mapping:
                markdown_content = self.html_converter.update_media_links(markdown_content, media_mapping)

            # ç”Ÿæˆæ–‡ä»¶å
            if not output_filename:
                output_filename = self.generate_filename(page_info['title'], url)

            # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€ï¼Œé¿å…è¦†ç›–
            output_filename = self.get_unique_filename(output_filename)
            output_path = self.output_dir / output_filename

            # æ„å»ºå®Œæ•´æ–‡æ¡£
            full_markdown = self.build_markdown_document(page_info, url, markdown_content)

            # ä¿å­˜æ–‡ä»¶
            output_path.write_text(full_markdown, encoding='utf-8')
            logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {output_path}")

            return output_path

        except Exception as e:
            logger.error(f"è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")
            return None

    async def convert_urls_from_clipboard(self, download_media: bool = True) -> List[Path]:
        """
        ä»å‰ªè´´æ¿è¯»å–URLå¹¶æ‰¹é‡è½¬æ¢ä¸ºMarkdownæ–‡ä»¶

        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
        :return: æˆåŠŸè½¬æ¢çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        urls, collected_notes = self.clipboard_manager.read_urls_from_clipboard()

        if collected_notes:
            md_content = "\n\n%%%\n\n".join(collected_notes)
            # ä¿å­˜æ–‡ä»¶
            collected_notes_filename = f"ç¢ç¬”è®°{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
            collected_notes_filename = self.get_unique_filename(collected_notes_filename)
            collected_notes_md_path = self.output_dir / collected_notes_filename
            collected_notes_md_path.write_text(md_content, encoding='utf-8')
            logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {collected_notes_md_path}")

        # logger.info(f"å¼€å§‹æ‰¹é‡è½¬æ¢ {len(urls)} ä¸ªURL...")
        successful_files = []
        if urls:
            for i, url in enumerate(urls, 1):
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é£ä¹¦é“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™æ˜¾ç¤ºç‰¹æ®Šæç¤º
                    if url.startswith("https://waytoagi.feishu.cn/"):
                        logger.info(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†é£ä¹¦é“¾æ¥: {url}")
                        logger.info("  ğŸ” æ£€æµ‹é£ä¹¦é“¾æ¥ï¼Œå°†è‡ªåŠ¨æå–å¾®ä¿¡åŸæ–‡é“¾æ¥...")
                    else:
                        logger.info(f"\n[{i}/{len(urls)}] æ­£åœ¨è½¬æ¢: {url}")

                    # è½¬æ¢URLä¸ºMarkdownï¼Œä¸æŒ‡å®šæ–‡ä»¶åï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ
                    result_path = await self.convert_url_to_markdown(
                        url=url,
                        output_filename=None,  # ä½¿ç”¨ç½‘é¡µæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                        download_media=download_media
                    )

                    if result_path:
                        successful_files.append(result_path)
                        file_size = result_path.stat().st_size
                        # logger.info(f"    âœ“ è½¬æ¢æˆåŠŸ! æ–‡ä»¶: {result_path.name} ({file_size:,} å­—èŠ‚)")
                    else:
                        logger.error(f"    âœ— è½¬æ¢å¤±è´¥: {url}")

                except Exception as e:
                    logger.error(f"    âœ— è½¬æ¢å‡ºé”™: {url}, é”™è¯¯: {e}")

            logger.info(f"\næ‰¹é‡è½¬æ¢å®Œæˆ! æˆåŠŸ: {len(successful_files)}/{len(urls)}")
        else:
            logger.warning("æ²¡æœ‰ä»å‰ªè´´æ¿æ‰¾åˆ°æœ‰æ•ˆçš„URL")

        return successful_files

    def is_clipboard_available(self) -> bool:
        """æ£€æŸ¥å‰ªè´´æ¿åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return self.clipboard_manager.is_available()