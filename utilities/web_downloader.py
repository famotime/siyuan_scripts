"""
ç½‘é¡µä¸‹è½½å™¨æ¨¡å—
è´Ÿè´£è·å–ç½‘é¡µå†…å®¹å¹¶æå–é¡µé¢ä¿¡æ¯
"""

import requests
import logging
import re
import html
import gzip
import zlib
from typing import Optional, Dict
from urllib.parse import urlparse

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    import brotli
    HAS_BROTLI = True
except ImportError:
    HAS_BROTLI = False

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

logger = logging.getLogger(__name__)


class WebDownloader:
    """ç½‘é¡µä¸‹è½½å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ç½‘é¡µä¸‹è½½å™¨"""
        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„ç‰¹æ®Šè¯·æ±‚å¤´
        self.site_specific_headers = {
            'toutiao.com': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            }
        }

    def _get_site_specific_headers(self, url: str) -> Dict[str, str]:
        """è·å–é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„è¯·æ±‚å¤´"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()

        # æ£€æŸ¥æ˜¯å¦æœ‰é’ˆå¯¹è¯¥åŸŸåçš„ç‰¹æ®Šè¯·æ±‚å¤´
        for site_pattern, headers in self.site_specific_headers.items():
            if site_pattern in domain:
                return {**self.headers, **headers}

        return self.headers

    def _decompress_content(self, content: bytes, encoding: str) -> bytes:
        """è§£å‹ç¼©å†…å®¹"""
        try:
            if encoding == 'gzip':
                return gzip.decompress(content)
            elif encoding == 'deflate':
                return zlib.decompress(content)
            elif encoding == 'br' and HAS_BROTLI:
                return brotli.decompress(content)
            else:
                return content
        except Exception as e:
            logger.warning(f"è§£å‹ç¼©å¤±è´¥ ({encoding}): {e}")
            return content

    def _detect_encoding(self, content: bytes, response_encoding: str = None) -> str:
        """æ£€æµ‹å†…å®¹ç¼–ç """
        # é¦–å…ˆå°è¯•å“åº”å¤´ä¸­çš„ç¼–ç 
        if response_encoding and response_encoding.lower() not in ['iso-8859-1', 'ascii']:
            return response_encoding

        # ä½¿ç”¨chardetæ£€æµ‹
        if HAS_CHARDET:
            detected = chardet.detect(content)
            if detected['encoding'] and detected['confidence'] > 0.7:
                return detected['encoding']

        # å°è¯•ä»HTML metaæ ‡ç­¾ä¸­æå–ç¼–ç 
        try:
            content_str = content.decode('utf-8', errors='ignore')
            charset_match = re.search(r'<meta[^>]*charset[="\s]*([^">\s]+)', content_str, re.IGNORECASE)
            if charset_match:
                return charset_match.group(1)
        except:
            pass

        # æŒ‰ä¼˜å…ˆçº§å°è¯•å¸¸è§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin-1']
        for encoding in encodings:
            try:
                content.decode(encoding)
                return encoding
            except UnicodeDecodeError:
                continue

        return 'utf-8'  # é»˜è®¤ä½¿ç”¨UTF-8

    def _extract_wechat_original_link(self, content: str) -> Optional[str]:
        """
        ä»é£ä¹¦é¡µé¢å†…å®¹ä¸­æå–å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥

        :param content: ç½‘é¡µHTMLå†…å®¹
        :return: å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        try:
            # é¦–å…ˆå°è¯•åœ¨å‰10è¡Œä¸­æœç´¢åŸæ–‡é“¾æ¥æ ¼å¼
            lines = content.split('\n')[:10]

            # åœ¨å‰10è¡Œä¸­æœç´¢åŸæ–‡é“¾æ¥
            for line in lines:
                # åŒ¹é…å„ç§å¯èƒ½çš„åŸæ–‡é“¾æ¥æ ¼å¼ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
                patterns = [
                    r'åŸæ–‡é“¾æ¥[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)',
                    r'åŸæ–‡é“¾æ¥\s*[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)',
                    r'åŸæ–‡é“¾æ¥\s+[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)',
                    r'åŸæ–‡é“¾æ¥[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"\s<>\']+)',
                    r'åŸæ–‡é“¾æ¥\s*[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"\s<>\']+)',
                    r'åŸæ–‡é“¾æ¥\s+[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"\s<>\']+)',
                ]

                for pattern in patterns:
                    match = re.search(pattern, line)
                    if match:
                        original_url = match.group(1)
                        # æ£€æŸ¥é“¾æ¥æ˜¯å¦è¢«æˆªæ–­ï¼Œå¦‚æœæ˜¯åˆ™è·³è¿‡
                        if '...' in original_url or len(original_url) < 40:
                            continue
                        logger.info(f"åœ¨é£ä¹¦é¡µé¢ä¸­æ‰¾åˆ°å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥: {original_url}")
                        return original_url

            # å¦‚æœå‰10è¡Œæ²¡æ‰¾åˆ°ï¼Œæœç´¢æ•´ä¸ªé¡µé¢ä¸­çš„å®Œæ•´å¾®ä¿¡é“¾æ¥
            # è¿™æ˜¯ä¸ºäº†å¤„ç†åŠ¨æ€åŠ è½½å†…å®¹çš„æƒ…å†µ
            wechat_patterns = [
                r'https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+',  # çŸ­é“¾æ¥æ ¼å¼
                r'https://mp\.weixin\.qq\.com/s\?[^"\s<>\']+',    # é•¿é“¾æ¥æ ¼å¼
            ]

            all_matches = []
            for pattern in wechat_patterns:
                matches = re.findall(pattern, content)
                all_matches.extend(matches)

            if all_matches:
                # å»é‡
                unique_matches = list(set(all_matches))

                # ä¼˜å…ˆçº§æ’åºï¼š
                # 1. è¿‡æ»¤æ‰æ˜æ˜¾è¢«æˆªæ–­çš„é“¾æ¥ï¼ˆåŒ…å«...æˆ–é•¿åº¦è¿‡çŸ­ï¼‰
                # 2. é€‰æ‹©æœ€é•¿çš„é“¾æ¥ï¼ˆé€šå¸¸æ˜¯å®Œæ•´é“¾æ¥ï¼‰
                # 3. ä¼˜å…ˆé€‰æ‹©åŒ…å«å®Œæ•´IDçš„é“¾æ¥

                valid_matches = []
                for link in unique_matches:
                    # è¿‡æ»¤æ¡ä»¶ï¼šä¸åŒ…å«...ï¼Œé•¿åº¦å¤§äº40ï¼Œä¸ä»¥...ç»“å°¾
                    if ('...' not in link and
                        len(link) > 40 and
                        not link.endswith('...')):
                        valid_matches.append(link)

                if valid_matches:
                    # åœ¨æœ‰æ•ˆé“¾æ¥ä¸­é€‰æ‹©æœ€é•¿çš„
                    original_url = max(valid_matches, key=len)
                else:
                    # å¦‚æœæ²¡æœ‰æœ‰æ•ˆé“¾æ¥ï¼Œé€‰æ‹©æœ€é•¿çš„ï¼ˆå¯èƒ½è¢«æˆªæ–­ï¼‰
                    original_url = max(unique_matches, key=len)

                logger.info(f"åœ¨é£ä¹¦é¡µé¢ä¸­æ‰¾åˆ°å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥: {original_url}")
                return original_url

            return None

        except Exception as e:
            logger.error(f"æå–å¾®ä¿¡åŸæ–‡é“¾æ¥æ—¶å‡ºé”™: {e}")
            return None

    def fetch_webpage(self, url: str) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹

        :param url: ç½‘é¡µURL
        :return: ç½‘é¡µHTMLå†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é£ä¹¦é“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™å…ˆå°è¯•æå–åŸæ–‡é“¾æ¥
            if url.startswith("https://waytoagi.feishu.cn/"):
                logger.info(f"æ£€æµ‹åˆ°é£ä¹¦é“¾æ¥ï¼Œå°è¯•æå–å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥: {url}")

                # å…ˆè·å–é£ä¹¦é¡µé¢å†…å®¹
                headers = self._get_site_specific_headers(url)
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                # è·å–åŸå§‹å†…å®¹å¹¶è§£ç 
                raw_content = response.content
                content_encoding = response.headers.get('Content-Encoding', '').lower()
                if content_encoding:
                    raw_content = self._decompress_content(raw_content, content_encoding)

                detected_encoding = self._detect_encoding(raw_content, response.encoding)
                try:
                    feishu_content = raw_content.decode(detected_encoding)
                except UnicodeDecodeError:
                    try:
                        feishu_content = raw_content.decode('utf-8', errors='replace')
                    except:
                        feishu_content = raw_content.decode('latin-1')

                # å°è¯•æå–å¾®ä¿¡åŸæ–‡é“¾æ¥
                original_url = self._extract_wechat_original_link(feishu_content)

                if original_url:
                    logger.info(f"  âœ… æ‰¾åˆ°å¾®ä¿¡åŸæ–‡é“¾æ¥: {original_url}")
                    logger.info(f"  ğŸ”„ é‡å®šå‘åˆ°åŸæ–‡é“¾æ¥è·å–å†…å®¹...")
                    # é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨åŸæ–‡é“¾æ¥è·å–å†…å®¹
                    return self.fetch_webpage(original_url)
                else:
                    logger.info("  âŒ æœªæ‰¾åˆ°å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥ï¼Œä½¿ç”¨é£ä¹¦é¡µé¢å†…å®¹")
                    return feishu_content

            # è·å–é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„è¯·æ±‚å¤´
            headers = self._get_site_specific_headers(url)

            # logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            # è·å–åŸå§‹å†…å®¹
            raw_content = response.content

            # å¤„ç†å‹ç¼©å†…å®¹
            content_encoding = response.headers.get('Content-Encoding', '').lower()
            if content_encoding:
                raw_content = self._decompress_content(raw_content, content_encoding)

            # æ£€æµ‹ç¼–ç 
            detected_encoding = self._detect_encoding(raw_content, response.encoding)

            # è§£ç å†…å®¹
            try:
                content = raw_content.decode(detected_encoding)
            except UnicodeDecodeError:
                # å¦‚æœæ£€æµ‹çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•UTF-8
                try:
                    content = raw_content.decode('utf-8', errors='replace')
                except:
                    content = raw_content.decode('latin-1')

            # æ£€æŸ¥æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢ï¼ˆå¦‚ä»Šæ—¥å¤´æ¡ï¼‰
            if self._is_js_redirect_page(content):
                logger.warning(f"æ£€æµ‹åˆ°JavaScripté‡å®šå‘é¡µé¢: {url}")
                # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ ç‰¹æ®Šå¤„ç†é€»è¾‘

            # logger.info(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return content

        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–ç½‘é¡µå¤±è´¥: {e}")
            return None

    def _is_js_redirect_page(self, content: str) -> bool:
        """æ£€æµ‹æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢"""
        # æ£€æŸ¥å†…å®¹é•¿åº¦å’Œç‰¹å¾
        if len(content) < 1000:
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸»è¦åŒ…å«JavaScriptä»£ç è€Œç¼ºå°‘å®é™…å†…å®¹
        js_indicators = [
            'window._$jsvmprt',
            'var glb;',
            'function(b,e,f)',
            'typeof window?global:window'
        ]

        js_count = sum(1 for indicator in js_indicators if indicator in content)

        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å®é™…çš„HTMLå†…å®¹ç»“æ„
        html_indicators = ['<article', '<main', '<div class="content', '<p>']
        html_count = sum(1 for indicator in html_indicators if indicator in content)

        return js_count >= 2 and html_count == 0

    def extract_page_info(self, html_content: str) -> Dict[str, str]:
        """
        æå–é¡µé¢åŸºæœ¬ä¿¡æ¯

        :param html_content: HTMLå†…å®¹
        :return: é¡µé¢ä¿¡æ¯å­—å…¸
        """
        if not HAS_BS4:
            logger.warning("BeautifulSoupæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._extract_page_info_simple(html_content)

        soup = BeautifulSoup(html_content, 'html.parser')

        # æå–æ ‡é¢˜
        title = "æœªçŸ¥æ ‡é¢˜"
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            title = title_tag.string.strip()
        else:
            h1_tag = soup.find('h1')
            if h1_tag:
                title = h1_tag.get_text().strip()

        # æ¸…ç†æ ‡é¢˜
        title = re.sub(r'\s+', ' ', title)

        # æå–æè¿°
        description = ""
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag and desc_tag.get('content'):
            description = desc_tag.get('content').strip()
        else:
            og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc_tag and og_desc_tag.get('content'):
                description = og_desc_tag.get('content').strip()

        # æå–ä½œè€…
        author = ""
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag and author_tag.get('content'):
            author = author_tag.get('content').strip()

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = ""
        time_selectors = [
            'meta[property="article:published_time"]',
            'meta[name="publish_date"]',
            'time[datetime]',
            '.publish-time',
            '.date'
        ]

        for selector in time_selectors:
            element = soup.select_one(selector)
            if element:
                if element.get('content'):
                    publish_time = element.get('content')
                elif element.get('datetime'):
                    publish_time = element.get('datetime')
                else:
                    publish_time = element.get_text().strip()
                break

        return {
            'title': title,
            'description': description,
            'author': author,
            'publish_time': publish_time
        }

    def _extract_page_info_simple(self, html_content: str) -> Dict[str, str]:
        """ç®€å•çš„é¡µé¢ä¿¡æ¯æå–ï¼ˆä¸ä½¿ç”¨BeautifulSoupï¼‰"""
        # æå–æ ‡é¢˜
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else "æœªçŸ¥æ ‡é¢˜"
        title = html.unescape(re.sub(r'\s+', ' ', title))

        # æå–æè¿°
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', html_content, re.IGNORECASE)
        description = html.unescape(desc_match.group(1).strip()) if desc_match else ""

        return {
            'title': title,
            'description': description,
            'author': "",
            'publish_time': ""
        }