"""
å°†è¾“å…¥çš„Webé¡µé¢é“¾æ¥è½¬æ¢ä¸ºMarkdownæ–‡æ¡£
åŸºäºæˆç†Ÿçš„åº“å®ç°ï¼Œæ”¯æŒåª’ä½“æ–‡ä»¶æœ¬åœ°ä¿å­˜
"""

import requests
import asyncio
import aiohttp
import aiofiles
from pathlib import Path
from typing import Optional, Dict, Any, List, Set, Tuple
import logging
import re
import html
from urllib.parse import urljoin, urlparse, unquote
from datetime import datetime
import mimetypes
import hashlib

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    import markdownify
    HAS_MARKDOWNIFY = True
except ImportError:
    HAS_MARKDOWNIFY = False

try:
    import html2text
    HAS_HTML2TEXT = True
except ImportError:
    HAS_HTML2TEXT = False

try:
    from PIL import Image
    HAS_PIL = True
except ImportError:
    HAS_PIL = False

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


class MediaDownloader:
    """åª’ä½“æ–‡ä»¶ä¸‹è½½å™¨"""

    def __init__(self, base_dir: Path, max_workers: int = 5):
        """
        åˆå§‹åŒ–åª’ä½“ä¸‹è½½å™¨

        :param base_dir: åŸºç¡€ç›®å½•
        :param max_workers: æœ€å¤§å¹¶å‘ä¸‹è½½æ•°
        """
        self.base_dir = base_dir
        self.media_dir = base_dir / "media"
        self.media_dir.mkdir(exist_ok=True)
        self.max_workers = max_workers

        # æ”¯æŒçš„åª’ä½“ç±»å‹
        self.supported_image_types = {'.jpg', '.jpeg', '.png', '.gif', '.webp', '.svg', '.bmp', '.ico'}
        self.supported_video_types = {'.mp4', '.webm', '.ogg', '.avi', '.mov', '.wmv', '.flv'}
        self.supported_audio_types = {'.mp3', '.wav', '.ogg', '.m4a', '.aac', '.flac'}

        # è¯·æ±‚å¤´
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': '*/*',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Connection': 'keep-alive',
        }

    def get_file_extension(self, url: str, content_type: str = None) -> str:
        """
        è·å–æ–‡ä»¶æ‰©å±•å

        :param url: æ–‡ä»¶URL
        :param content_type: MIMEç±»å‹
        :return: æ–‡ä»¶æ‰©å±•å
        """
        # ä»URLè·å–æ‰©å±•å
        parsed_url = urlparse(url)
        path = unquote(parsed_url.path)
        ext = Path(path).suffix.lower()

        if ext and ext in (self.supported_image_types | self.supported_video_types | self.supported_audio_types):
            return ext

        # ä»Content-Typeè·å–æ‰©å±•å
        if content_type:
            ext = mimetypes.guess_extension(content_type.split(';')[0])
            if ext:
                return ext.lower()

        # é»˜è®¤æ‰©å±•å
        if content_type:
            if content_type.startswith('image/'):
                return '.jpg'
            elif content_type.startswith('video/'):
                return '.mp4'
            elif content_type.startswith('audio/'):
                return '.mp3'

        return '.bin'

    def generate_filename(self, url: str, content_type: str = None) -> str:
        """
        ç”Ÿæˆå”¯ä¸€çš„æ–‡ä»¶å

        :param url: åŸå§‹URL
        :param content_type: MIMEç±»å‹
        :return: æ–‡ä»¶å
        """
        # ä½¿ç”¨URLçš„å“ˆå¸Œå€¼ç”Ÿæˆå”¯ä¸€æ–‡ä»¶å
        url_hash = hashlib.md5(url.encode('utf-8')).hexdigest()[:12]
        ext = self.get_file_extension(url, content_type)

        # å°è¯•ä»URLè·å–æœ‰æ„ä¹‰çš„æ–‡ä»¶å
        parsed_url = urlparse(url)
        original_name = Path(unquote(parsed_url.path)).stem
        if original_name and len(original_name) > 0 and original_name != '/':
            # æ¸…ç†æ–‡ä»¶å
            clean_name = re.sub(r'[^\w\-_.]', '_', original_name)[:30]
            return f"{clean_name}_{url_hash}{ext}"

        return f"media_{url_hash}{ext}"

    async def download_media_async(self, session: aiohttp.ClientSession, url: str, base_url: str = "") -> Optional[Tuple[str, str]]:
        """
        å¼‚æ­¥ä¸‹è½½å•ä¸ªåª’ä½“æ–‡ä»¶

        :param session: aiohttpä¼šè¯
        :param url: åª’ä½“æ–‡ä»¶URL
        :param base_url: åŸºç¡€URL
        :return: (æœ¬åœ°æ–‡ä»¶è·¯å¾„, ç›¸å¯¹è·¯å¾„) æˆ– None
        """
        try:
            # å¤„ç†ç›¸å¯¹URL
            if base_url:
                full_url = urljoin(base_url, url)
            else:
                full_url = url

            logger.info(f"ä¸‹è½½åª’ä½“æ–‡ä»¶: {full_url}")

            async with session.get(full_url, headers=self.headers) as response:
                if response.status != 200:
                    logger.warning(f"ä¸‹è½½å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status}, URL: {full_url}")
                    return None

                content_type = response.headers.get('content-type', '')

                # æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„åª’ä½“ç±»å‹
                if not self.is_supported_media_type(content_type, full_url):
                    logger.info(f"è·³è¿‡ä¸æ”¯æŒçš„åª’ä½“ç±»å‹: {content_type}, URL: {full_url}")
                    return None

                # ç”Ÿæˆæ–‡ä»¶å
                filename = self.generate_filename(full_url, content_type)
                file_path = self.media_dir / filename

                # å¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œç›´æ¥è¿”å›
                if file_path.exists():
                    logger.info(f"æ–‡ä»¶å·²å­˜åœ¨ï¼Œè·³è¿‡ä¸‹è½½: {filename}")
                    return str(file_path), f"media/{filename}"

                # ä¸‹è½½æ–‡ä»¶
                content = await response.read()

                # éªŒè¯æ–‡ä»¶å¤§å°
                if len(content) < 100:  # å¤ªå°çš„æ–‡ä»¶å¯èƒ½ä¸æ˜¯æœ‰æ•ˆåª’ä½“
                    logger.warning(f"æ–‡ä»¶å¤ªå°ï¼Œå¯èƒ½æ— æ•ˆ: {len(content)} bytes, URL: {full_url}")
                    return None

                # ä¿å­˜æ–‡ä»¶
                async with aiofiles.open(file_path, 'wb') as f:
                    await f.write(content)

                logger.info(f"åª’ä½“æ–‡ä»¶å·²ä¿å­˜: {filename} ({len(content)} bytes)")

                # å¦‚æœæ˜¯å›¾ç‰‡ï¼Œå°è¯•éªŒè¯å’Œä¼˜åŒ–
                if content_type.startswith('image/') and HAS_PIL:
                    try:
                        # éªŒè¯å›¾ç‰‡
                        with Image.open(file_path) as img:
                            # å¯ä»¥åœ¨è¿™é‡Œæ·»åŠ å›¾ç‰‡ä¼˜åŒ–é€»è¾‘
                            pass
                    except Exception as e:
                        logger.warning(f"å›¾ç‰‡éªŒè¯å¤±è´¥: {filename}, é”™è¯¯: {e}")

                return str(file_path), f"media/{filename}"

        except Exception as e:
            logger.error(f"ä¸‹è½½åª’ä½“æ–‡ä»¶å¤±è´¥: {url}, é”™è¯¯: {e}")
            return None

    def is_supported_media_type(self, content_type: str, url: str) -> bool:
        """
        æ£€æŸ¥æ˜¯å¦ä¸ºæ”¯æŒçš„åª’ä½“ç±»å‹

        :param content_type: MIMEç±»å‹
        :param url: æ–‡ä»¶URL
        :return: æ˜¯å¦æ”¯æŒ
        """
        # æ£€æŸ¥Content-Type
        if content_type:
            if (content_type.startswith('image/') or
                content_type.startswith('video/') or
                content_type.startswith('audio/')):
                return True

        # æ£€æŸ¥æ–‡ä»¶æ‰©å±•å
        ext = Path(urlparse(url).path).suffix.lower()
        return ext in (self.supported_image_types | self.supported_video_types | self.supported_audio_types)

    async def download_media_batch(self, urls: List[str], base_url: str = "") -> Dict[str, str]:
        """
        æ‰¹é‡ä¸‹è½½åª’ä½“æ–‡ä»¶

        :param urls: åª’ä½“æ–‡ä»¶URLåˆ—è¡¨
        :param base_url: åŸºç¡€URL
        :return: URLåˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
        """
        url_to_local = {}

        if not urls:
            return url_to_local

        # åˆ›å»ºå¼‚æ­¥ä¼šè¯
        connector = aiohttp.TCPConnector(limit=self.max_workers)
        timeout = aiohttp.ClientTimeout(total=30, connect=10)

        async with aiohttp.ClientSession(connector=connector, timeout=timeout) as session:
            # åˆ›å»ºä¸‹è½½ä»»åŠ¡
            tasks = []
            for url in urls:
                task = asyncio.create_task(self.download_media_async(session, url, base_url))
                tasks.append((url, task))

            # æ‰§è¡Œä¸‹è½½
            for original_url, task in tasks:
                try:
                    result = await task
                    if result:
                        local_path, relative_path = result
                        url_to_local[original_url] = relative_path
                except Exception as e:
                    logger.error(f"ä¸‹è½½ä»»åŠ¡å¤±è´¥: {original_url}, é”™è¯¯: {e}")

        return url_to_local


class AdvancedWebToMarkdownConverter:
    """é«˜çº§Webé¡µé¢è½¬Markdownè½¬æ¢å™¨"""

    def __init__(self, output_dir: str = "output", converter_lib: str = "auto"):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨

        :param output_dir: è¾“å‡ºç›®å½•
        :param converter_lib: è½¬æ¢åº“é€‰æ‹© ("markdownify", "html2text", "auto")
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # é€‰æ‹©è½¬æ¢åº“
        self.converter_lib = self._select_converter_lib(converter_lib)
        logger.info(f"ä½¿ç”¨è½¬æ¢åº“: {self.converter_lib}")

        # åˆå§‹åŒ–åª’ä½“ä¸‹è½½å™¨
        self.media_downloader = MediaDownloader(self.output_dir)

        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }

    def _select_converter_lib(self, preference: str) -> str:
        """é€‰æ‹©å¯ç”¨çš„è½¬æ¢åº“"""
        if preference == "markdownify" and HAS_MARKDOWNIFY:
            return "markdownify"
        elif preference == "html2text" and HAS_HTML2TEXT:
            return "html2text"
        elif preference == "auto":
            if HAS_MARKDOWNIFY:
                return "markdownify"
            elif HAS_HTML2TEXT:
                return "html2text"
            else:
                return "builtin"
        else:
            return "builtin"

    def fetch_webpage(self, url: str) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹

        :param url: ç½‘é¡µURL
        :return: ç½‘é¡µHTMLå†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")

            response = requests.get(url, headers=self.headers, timeout=30)
            response.raise_for_status()

            # å°è¯•æ£€æµ‹ç¼–ç 
            if response.encoding == 'ISO-8859-1' or response.encoding is None:
                encodings = ['utf-8', 'gbk', 'gb2312', 'latin-1']
                for encoding in encodings:
                    try:
                        response.encoding = encoding
                        content = response.text
                        content.encode('utf-8')
                        break
                    except (UnicodeDecodeError, UnicodeEncodeError):
                        continue
                else:
                    response.encoding = 'utf-8'

            logger.info(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œå¤§å°: {len(response.text)} å­—ç¬¦")
            return response.text

        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–ç½‘é¡µå¤±è´¥: {e}")
            return None

    def extract_page_info(self, html_content: str) -> Dict[str, str]:
        """
        æå–é¡µé¢åŸºæœ¬ä¿¡æ¯

        :param html_content: HTMLå†…å®¹
        :return: é¡µé¢ä¿¡æ¯å­—å…¸
        """
        if not HAS_BS4:
            logger.warning("BeautifulSoupæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._extract_page_info_simple(html_content)

        soup = BeautifulSoup(html_content, 'html.parser')

        # æå–æ ‡é¢˜
        title = "æœªçŸ¥æ ‡é¢˜"
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            title = title_tag.string.strip()
        else:
            h1_tag = soup.find('h1')
            if h1_tag:
                title = h1_tag.get_text().strip()

        # æ¸…ç†æ ‡é¢˜
        title = re.sub(r'\s+', ' ', title)

        # æå–æè¿°
        description = ""
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag and desc_tag.get('content'):
            description = desc_tag.get('content').strip()
        else:
            og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc_tag and og_desc_tag.get('content'):
                description = og_desc_tag.get('content').strip()

        # æå–ä½œè€…
        author = ""
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag and author_tag.get('content'):
            author = author_tag.get('content').strip()

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = ""
        time_selectors = [
            'meta[property="article:published_time"]',
            'meta[name="publish_date"]',
            'time[datetime]',
            '.publish-time',
            '.date'
        ]

        for selector in time_selectors:
            element = soup.select_one(selector)
            if element:
                if element.get('content'):
                    publish_time = element.get('content')
                elif element.get('datetime'):
                    publish_time = element.get('datetime')
                else:
                    publish_time = element.get_text().strip()
                break

        return {
            'title': title,
            'description': description,
            'author': author,
            'publish_time': publish_time
        }

    def _extract_page_info_simple(self, html_content: str) -> Dict[str, str]:
        """ç®€å•çš„é¡µé¢ä¿¡æ¯æå–ï¼ˆä¸ä½¿ç”¨BeautifulSoupï¼‰"""
        # æå–æ ‡é¢˜
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else "æœªçŸ¥æ ‡é¢˜"
        title = html.unescape(re.sub(r'\s+', ' ', title))

        # æå–æè¿°
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', html_content, re.IGNORECASE)
        description = html.unescape(desc_match.group(1).strip()) if desc_match else ""

        return {
            'title': title,
            'description': description,
            'author': "",
            'publish_time': ""
        }

    def extract_media_urls(self, html_content: str, base_url: str) -> List[str]:
        """
        æå–é¡µé¢ä¸­çš„åª’ä½“æ–‡ä»¶URL

        :param html_content: HTMLå†…å®¹
        :param base_url: åŸºç¡€URL
        :return: åª’ä½“æ–‡ä»¶URLåˆ—è¡¨
        """
        media_urls = set()

        if HAS_BS4:
            soup = BeautifulSoup(html_content, 'html.parser')

            # æå–å›¾ç‰‡
            for img in soup.find_all('img'):
                src = img.get('src') or img.get('data-src') or img.get('data-original')
                if src:
                    full_url = urljoin(base_url, src)
                    media_urls.add(full_url)

            # æå–è§†é¢‘
            for video in soup.find_all('video'):
                src = video.get('src')
                if src:
                    full_url = urljoin(base_url, src)
                    media_urls.add(full_url)

                # æå–videoæ ‡ç­¾å†…çš„source
                for source in video.find_all('source'):
                    src = source.get('src')
                    if src:
                        full_url = urljoin(base_url, src)
                        media_urls.add(full_url)

            # æå–éŸ³é¢‘
            for audio in soup.find_all('audio'):
                src = audio.get('src')
                if src:
                    full_url = urljoin(base_url, src)
                    media_urls.add(full_url)

                # æå–audioæ ‡ç­¾å†…çš„source
                for source in audio.find_all('source'):
                    src = source.get('src')
                    if src:
                        full_url = urljoin(base_url, src)
                        media_urls.add(full_url)
        else:
            # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æå–
            img_pattern = r'<img[^>]*src=["\']([^"\']*)["\']'
            for match in re.finditer(img_pattern, html_content, re.IGNORECASE):
                full_url = urljoin(base_url, match.group(1))
                media_urls.add(full_url)

            video_pattern = r'<video[^>]*src=["\']([^"\']*)["\']'
            for match in re.finditer(video_pattern, html_content, re.IGNORECASE):
                full_url = urljoin(base_url, match.group(1))
                media_urls.add(full_url)

        return list(media_urls)

    def clean_html_for_conversion(self, html_content: str) -> str:
        """
        æ¸…ç†HTMLå†…å®¹ç”¨äºè½¬æ¢

        :param html_content: åŸå§‹HTML
        :return: æ¸…ç†åçš„HTML
        """
        if not HAS_BS4:
            return self._clean_html_simple(html_content)

        soup = BeautifulSoup(html_content, 'html.parser')

        # ç§»é™¤ä¸éœ€è¦çš„æ ‡ç­¾
        unwanted_tags = [
            'script', 'style', 'nav', 'header', 'footer', 'aside',
            'noscript', 'iframe', 'object', 'embed', 'form',
            'button', 'input', 'select', 'textarea'
        ]

        for tag_name in unwanted_tags:
            for tag in soup.find_all(tag_name):
                tag.decompose()

        # ç§»é™¤æ³¨é‡Š
        from bs4 import Comment
        for comment in soup.find_all(string=lambda text: isinstance(text, Comment)):
            comment.extract()

        # å°è¯•æå–ä¸»è¦å†…å®¹
        main_content = None
        content_selectors = [
            'main', 'article', '[role="main"]',
            '.content', '#content', '.main', '#main',
            '.post-content', '.entry-content', '.article-content',
            '.post-body', '.entry-body'
        ]

        for selector in content_selectors:
            element = soup.select_one(selector)
            if element and len(element.get_text().strip()) > 200:
                main_content = element
                break

        if main_content:
            return str(main_content)
        else:
            # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä¸»è¦å†…å®¹ï¼Œä½¿ç”¨body
            body = soup.find('body')
            return str(body) if body else str(soup)

    def _clean_html_simple(self, html_content: str) -> str:
        """ç®€å•çš„HTMLæ¸…ç†"""
        unwanted_patterns = [
            r'<script[^>]*>.*?</script>',
            r'<style[^>]*>.*?</style>',
            r'<nav[^>]*>.*?</nav>',
            r'<header[^>]*>.*?</header>',
            r'<footer[^>]*>.*?</footer>',
            r'<aside[^>]*>.*?</aside>',
            r'<!--.*?-->',
        ]

        content = html_content
        for pattern in unwanted_patterns:
            content = re.sub(pattern, '', content, flags=re.IGNORECASE | re.DOTALL)

        return content

    def convert_html_to_markdown(self, html_content: str) -> str:
        """
        å°†HTMLè½¬æ¢ä¸ºMarkdown

        :param html_content: HTMLå†…å®¹
        :return: Markdownå†…å®¹
        """
        if self.converter_lib == "markdownify":
            return self._convert_with_markdownify(html_content)
        elif self.converter_lib == "html2text":
            return self._convert_with_html2text(html_content)
        else:
            return self._convert_builtin(html_content)

    def _convert_with_markdownify(self, html_content: str) -> str:
        """ä½¿ç”¨markdownifyè½¬æ¢"""
        try:
            markdown_content = markdownify.markdownify(
                html_content,
                heading_style=markdownify.ATX,
                bullets='-',
                strong_tag='**',
                em_tag='*'
            )
            return self._clean_markdown(markdown_content)
        except Exception as e:
            logger.error(f"markdownifyè½¬æ¢å¤±è´¥: {e}")
            return self._convert_builtin(html_content)

    def _convert_with_html2text(self, html_content: str) -> str:
        """ä½¿ç”¨html2textè½¬æ¢"""
        try:
            h = html2text.HTML2Text()
            h.ignore_links = False
            h.ignore_images = False
            h.body_width = 0  # ä¸é™åˆ¶è¡Œå®½
            h.unicode_snob = True
            h.skip_internal_links = True

            markdown_content = h.handle(html_content)
            return self._clean_markdown(markdown_content)
        except Exception as e:
            logger.error(f"html2textè½¬æ¢å¤±è´¥: {e}")
            return self._convert_builtin(html_content)

    def _convert_builtin(self, html_content: str) -> str:
        """å†…ç½®è½¬æ¢æ–¹æ³•"""
        content = html_content

        # å¤„ç†æ ‡é¢˜
        for i in range(6, 0, -1):
            content = re.sub(rf'<h{i}[^>]*>(.*?)</h{i}>', rf'{"#" * i} \1\n\n', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†æ®µè½
        content = re.sub(r'<p[^>]*>(.*?)</p>', r'\1\n\n', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†æ ¼å¼
        content = re.sub(r'<strong[^>]*>(.*?)</strong>', r'**\1**', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'<b[^>]*>(.*?)</b>', r'**\1**', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'<em[^>]*>(.*?)</em>', r'*\1*', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'<i[^>]*>(.*?)</i>', r'*\1*', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†é“¾æ¥
        content = re.sub(r'<a[^>]*href=["\']([^"\']*)["\'][^>]*>(.*?)</a>', r'[\2](\1)', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†å›¾ç‰‡
        content = re.sub(r'<img[^>]*src=["\']([^"\']*)["\'][^>]*alt=["\']([^"\']*)["\'][^>]*/?>', r'![\2](\1)', content, flags=re.IGNORECASE)
        content = re.sub(r'<img[^>]*src=["\']([^"\']*)["\'][^>]*/?>', r'![](\1)', content, flags=re.IGNORECASE)

        # å¤„ç†åˆ—è¡¨
        content = re.sub(r'<li[^>]*>(.*?)</li>', r'- \1\n', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'<[uo]l[^>]*>(.*?)</[uo]l>', r'\1\n', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†ä»£ç 
        content = re.sub(r'<code[^>]*>(.*?)</code>', r'`\1`', content, flags=re.IGNORECASE | re.DOTALL)
        content = re.sub(r'<pre[^>]*>(.*?)</pre>', r'```\n\1\n```\n', content, flags=re.IGNORECASE | re.DOTALL)

        # å¤„ç†æ¢è¡Œ
        content = re.sub(r'<br[^>]*/?>', '\n', content, flags=re.IGNORECASE)
        content = re.sub(r'<hr[^>]*/?>', '\n---\n', content, flags=re.IGNORECASE)

        # ç§»é™¤å‰©ä½™HTMLæ ‡ç­¾
        content = re.sub(r'<[^>]+>', '', content)

        return self._clean_markdown(content)

    def _clean_markdown(self, markdown_content: str) -> str:
        """æ¸…ç†Markdownå†…å®¹"""
        # è§£ç HTMLå®ä½“
        content = html.unescape(markdown_content)

        # è§„èŒƒåŒ–ç©ºç™½å­—ç¬¦
        content = re.sub(r'[ \t]+', ' ', content)
        content = re.sub(r'\n\s*\n\s*\n+', '\n\n', content)

        # æ¸…ç†è¡Œ
        lines = []
        for line in content.split('\n'):
            lines.append(line.rstrip())

        content = '\n'.join(lines).strip()
        return content

    def update_media_links(self, markdown_content: str, media_mapping: Dict[str, str]) -> str:
        """
        æ›´æ–°Markdownä¸­çš„åª’ä½“é“¾æ¥ä¸ºæœ¬åœ°è·¯å¾„

        :param markdown_content: åŸå§‹Markdownå†…å®¹
        :param media_mapping: URLåˆ°æœ¬åœ°è·¯å¾„çš„æ˜ å°„
        :return: æ›´æ–°åçš„Markdownå†…å®¹
        """
        content = markdown_content

        for original_url, local_path in media_mapping.items():
            # æ›´æ–°å›¾ç‰‡é“¾æ¥
            content = content.replace(f"]({original_url})", f"]({local_path})")
            content = content.replace(f'src="{original_url}"', f'src="{local_path}"')
            content = content.replace(f"src='{original_url}'", f"src='{local_path}'")

        return content

    def generate_filename(self, title: str, url: str) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å

        :param title: ç½‘é¡µæ ‡é¢˜
        :param url: ç½‘é¡µURL
        :return: å®‰å…¨çš„æ–‡ä»¶å
        """
        # æ¸…ç†æ ‡é¢˜ä½œä¸ºæ–‡ä»¶å
        filename = re.sub(r'[^\w\s\-_.]', '', title)
        filename = re.sub(r'[-\s]+', '-', filename)
        filename = filename.strip('-')

        # å¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨URLçš„åŸŸå
        if not filename or len(filename) < 3:
            parsed_url = urlparse(url)
            filename = parsed_url.netloc.replace('.', '-')

        # é™åˆ¶æ–‡ä»¶åé•¿åº¦
        if len(filename) > 50:
            filename = filename[:50]

        # æ·»åŠ æ—¶é—´æˆ³é¿å…é‡å¤
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        filename = f"{filename}_{timestamp}"

        return f"{filename}.md"

    def build_markdown_document(self, page_info: Dict[str, str], url: str, markdown_content: str) -> str:
        """
        æ„å»ºå®Œæ•´çš„Markdownæ–‡æ¡£

        :param page_info: é¡µé¢ä¿¡æ¯
        :param url: åŸå§‹URL
        :param markdown_content: è½¬æ¢åçš„å†…å®¹
        :return: å®Œæ•´çš„Markdownæ–‡æ¡£
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºæ–‡æ¡£å¤´éƒ¨
        header_parts = [f"# {page_info['title']}", ""]

        # æ·»åŠ å…ƒæ•°æ®
        metadata = [
            f"**åŸå§‹é“¾æ¥ï¼š** {url}",
            f"**è½¬æ¢æ—¶é—´ï¼š** {current_time}",
        ]

        if page_info['description']:
            metadata.append(f"**æè¿°ï¼š** {page_info['description']}")

        if page_info['author']:
            metadata.append(f"**ä½œè€…ï¼š** {page_info['author']}")

        if page_info['publish_time']:
            metadata.append(f"**å‘å¸ƒæ—¶é—´ï¼š** {page_info['publish_time']}")

        header_parts.extend(metadata)
        header_parts.extend(["", "---", ""])

        # ç»„åˆå®Œæ•´æ–‡æ¡£
        full_document = '\n'.join(header_parts) + markdown_content

        return full_document

    async def convert_url_to_markdown(self, url: str, output_filename: str = None, download_media: bool = True) -> Optional[Path]:
        """
        å°†URLè½¬æ¢ä¸ºMarkdownæ–‡ä»¶

        :param url: è¦è½¬æ¢çš„URL
        :param output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
        :return: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # è·å–ç½‘é¡µå†…å®¹
            html_content = self.fetch_webpage(url)
            if not html_content:
                return None

            # æå–é¡µé¢ä¿¡æ¯
            page_info = self.extract_page_info(html_content)
            logger.info(f"é¡µé¢æ ‡é¢˜: {page_info['title']}")

            # ä¸‹è½½åª’ä½“æ–‡ä»¶
            media_mapping = {}
            if download_media:
                logger.info("å¼€å§‹ä¸‹è½½åª’ä½“æ–‡ä»¶...")
                media_urls = self.extract_media_urls(html_content, url)
                logger.info(f"å‘ç° {len(media_urls)} ä¸ªåª’ä½“æ–‡ä»¶")

                if media_urls:
                    media_mapping = await self.media_downloader.download_media_batch(media_urls, url)
                    logger.info(f"æˆåŠŸä¸‹è½½ {len(media_mapping)} ä¸ªåª’ä½“æ–‡ä»¶")

            # æ¸…ç†HTMLå†…å®¹
            clean_html = self.clean_html_for_conversion(html_content)

            # è½¬æ¢ä¸ºMarkdown
            markdown_content = self.convert_html_to_markdown(clean_html)

            # æ›´æ–°åª’ä½“é“¾æ¥
            if media_mapping:
                markdown_content = self.update_media_links(markdown_content, media_mapping)

            # ç”Ÿæˆæ–‡ä»¶å
            if not output_filename:
                output_filename = self.generate_filename(page_info['title'], url)

            output_path = self.output_dir / output_filename

            # æ„å»ºå®Œæ•´æ–‡æ¡£
            full_markdown = self.build_markdown_document(page_info, url, markdown_content)

            # ä¿å­˜æ–‡ä»¶
            output_path.write_text(full_markdown, encoding='utf-8')
            logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {output_path}")

            return output_path

        except Exception as e:
            logger.error(f"è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")
            return None


async def main():
    """
    ä¸»å‡½æ•° - æä¾›ä½¿ç”¨ç¤ºä¾‹
    """
    print("=== URLè½¬Markdownè½¬æ¢å™¨ä½¿ç”¨ç¤ºä¾‹ ===\n")

    # ç¤ºä¾‹URLåˆ—è¡¨ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦ä¿®æ”¹ï¼‰
    demo_urls = [
        ("https://www.yuque.com/siyuannote/docs/go7uom#04ea747f", "æ€æºç¬”è®°æ•°æ®åº“.md")
    ]

    # åˆ›å»ºè½¬æ¢å™¨å®ä¾‹
    converter = AdvancedWebToMarkdownConverter(output_dir="demo_output")

    print("å¼€å§‹è½¬æ¢ç¤ºä¾‹ç½‘é¡µ...\n")

    success_count = 0
    total_count = len(demo_urls)

    for i, (url, filename) in enumerate(demo_urls, 1):
        print(f"[{i}/{total_count}] æ­£åœ¨è½¬æ¢: {url}")
        print(f"    è¾“å‡ºæ–‡ä»¶: {filename}")

        try:
            # è½¬æ¢ç½‘é¡µä¸ºMarkdown
            result_path = await converter.convert_url_to_markdown(
                url=url,
                output_filename=filename,
                download_media=True  # ä¸‹è½½åª’ä½“æ–‡ä»¶
            )

            if result_path:
                file_size = result_path.stat().st_size
                print(f"    âœ“ è½¬æ¢æˆåŠŸ! æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
                success_count += 1

                # æ£€æŸ¥åª’ä½“æ–‡ä»¶
                media_dir = converter.output_dir / "media"
                if media_dir.exists():
                    media_files = list(media_dir.glob("*"))
                    if media_files:
                        print(f"    ğŸ“ ä¸‹è½½äº† {len(media_files)} ä¸ªåª’ä½“æ–‡ä»¶")
            else:
                print("    âœ— è½¬æ¢å¤±è´¥")

        except Exception as e:
            print(f"    âœ— è½¬æ¢å‡ºé”™: {e}")

        print()  # ç©ºè¡Œåˆ†éš”

    # æ€»ç»“
    print("=" * 50)
    print(f"è½¬æ¢å®Œæˆ! æˆåŠŸ: {success_count}/{total_count}")
    print(f"è¾“å‡ºç›®å½•: {converter.output_dir.absolute()}")

    # æ˜¾ç¤ºè¾“å‡ºç›®å½•å†…å®¹
    if converter.output_dir.exists():
        markdown_files = list(converter.output_dir.glob("*.md"))
        media_dir = converter.output_dir / "media"

        print(f"\nç”Ÿæˆçš„Markdownæ–‡ä»¶:")
        for md_file in markdown_files:
            size = md_file.stat().st_size
            print(f"  ğŸ“„ {md_file.name} ({size:,} å­—èŠ‚)")

        if media_dir.exists():
            media_files = list(media_dir.glob("*"))
            if media_files:
                print(f"\nä¸‹è½½çš„åª’ä½“æ–‡ä»¶: {len(media_files)} ä¸ª")
                print(f"  ğŸ“ åª’ä½“ç›®å½•: {media_dir}")

    print("\nä½¿ç”¨è¯´æ˜:")
    print("1. å¯ä»¥ç›´æ¥å¯¼å…¥ AdvancedWebToMarkdownConverter ç±»")
    print("2. æ”¯æŒè‡ªå®šä¹‰è¾“å‡ºç›®å½•å’Œæ–‡ä»¶å")
    print("3. æ”¯æŒé€‰æ‹©ä¸åŒçš„è½¬æ¢åº“ (markdownify, html2text, auto)")
    print("4. æ”¯æŒå¼€å¯/å…³é—­åª’ä½“æ–‡ä»¶ä¸‹è½½")
    print("\nç¤ºä¾‹ä»£ç :")
    print("""
    from url2markdown import AdvancedWebToMarkdownConverter
    import asyncio

    async def convert_webpage():
        converter = AdvancedWebToMarkdownConverter(output_dir="my_output")
        result = await converter.convert_url_to_markdown(
            "https://example.com/",
            "example.md",
            download_media=True
        )
        return result

    # è¿è¡Œè½¬æ¢
    asyncio.run(convert_webpage())
    """)


if __name__ == "__main__":
    # è¿è¡Œç¤ºä¾‹
    asyncio.run(main())