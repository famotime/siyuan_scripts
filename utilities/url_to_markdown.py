"""
URLè½¬Markdownè½¬æ¢å™¨
æ•´åˆå„ä¸ªæ¨¡å—æä¾›å®Œæ•´çš„è½¬æ¢åŠŸèƒ½
"""

import asyncio
import logging
import time
from pathlib import Path
from typing import Optional, List, Dict
from datetime import datetime
import re
from functools import partial
from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse

from .web_downloader import WebDownloader
from .html_converter import HTMLConverter
from .media_downloader import MediaDownloader
from .clipboard_manager import ClipboardManager
from .special_site_handler import SpecialSiteHandler

logger = logging.getLogger(__name__)


class URLToMarkdownConverter:
    """URLè½¬Markdownè½¬æ¢å™¨"""

    def __init__(self, output_dir: str = "output", converter_lib: str = "auto"):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨

        :param output_dir: è¾“å‡ºç›®å½•
        :param converter_lib: è½¬æ¢åº“é€‰æ‹© ("markdownify", "html2text", "auto")
        """
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(exist_ok=True)

        # åˆå§‹åŒ–å„ä¸ªç»„ä»¶
        self.web_downloader = WebDownloader()
        self.html_converter = HTMLConverter(converter_lib)
        self.media_downloader = MediaDownloader(self.output_dir)
        self.clipboard_manager = ClipboardManager()
        self.special_site_handler = SpecialSiteHandler()
        self.url_timeout_seconds = 60
        self.last_error: Dict[str, str] = {}
        self._tracking_query_params = {
            "timestamp",
            "req_id",
            "req_id_new",
            "share_did",
            "share_uid",
            "share_token",
            "use_new_style",
            "from",
            "source",
            "wid",
            "utm_source",
            "utm_medium",
            "utm_campaign",
            "utm_term",
            "utm_content",
            "spm",
            "fbclid",
            "gclid",
            "yclid",
        }

    def _set_last_error(self, code: str, message: str, stage: str = "") -> None:
        """è®°å½•æœ€è¿‘ä¸€æ¬¡è½¬æ¢é”™è¯¯ä¿¡æ¯ï¼Œä¾¿äºæ‰¹å¤„ç†å›é€€ã€‚"""
        self.last_error = {
            "code": code,
            "message": message,
            "stage": stage,
            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        }

    def _remaining_seconds(self, start_time: float) -> float:
        """è¿”å›å½“å‰URLå‰©ä½™å¤„ç†é¢„ç®—ç§’æ•°ã€‚"""
        return self.url_timeout_seconds - (time.monotonic() - start_time)

    async def _run_blocking_with_timeout(self, func, *args, timeout: float):
        """åœ¨çº¿ç¨‹æ± æ‰§è¡Œé˜»å¡å‡½æ•°ï¼Œå¹¶å—é™äºå‰©ä½™é¢„ç®—ã€‚"""
        loop = asyncio.get_running_loop()
        return await asyncio.wait_for(loop.run_in_executor(None, partial(func, *args)), timeout=timeout)

    def _format_failed_url_note(self, item: Dict[str, str]) -> str:
        """æ ¼å¼åŒ–å¤±è´¥URLçš„äººå·¥å¤æ ¸æ¡ç›®ã€‚"""
        failed_url = item.get('url', '')
        lines = [
            "[å¾…äººå·¥éªŒè¯]",
            f"æ—¶é—´: {item.get('timestamp', '')}",
            f"URL: [{failed_url}]({failed_url})" if failed_url else "URL: ",
            f"é”™è¯¯: {item.get('message', '')}",
        ]
        if item.get("stage"):
            lines.append(f"é˜¶æ®µ: {item['stage']}")
        return "\n".join(lines)

    def _save_collected_notes(self, collected_notes: List[str], failed_items: List[Dict[str, str]]) -> None:
        """å°†ç¢ç¬”è®°å†…å®¹ä¸å¤±è´¥URLæ¡ç›®ç»Ÿä¸€å†™å…¥ç¢ç¬”è®°æ–‡ä»¶ã€‚"""
        sections: List[str] = []
        if collected_notes:
            sections.extend(collected_notes)
        if failed_items:
            sections.extend([self._format_failed_url_note(item) for item in failed_items])

        if not sections:
            return

        md_content = "\n\n%%%\n\n".join(sections)
        collected_notes_filename = f"ç¢ç¬”è®°{datetime.now().strftime('%Y%m%d_%H%M%S')}.md"
        collected_notes_filename = self.get_unique_filename(collected_notes_filename)
        collected_notes_md_path = self.output_dir / collected_notes_filename
        collected_notes_md_path.write_text(md_content, encoding='utf-8')
        logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {collected_notes_md_path}")

    def _build_dedup_key(self, raw_url: str) -> str:
        """æ„å»ºURLå»é‡é”®ï¼Œåˆå¹¶åŒä¸€èµ„æºçš„ä¸åŒåˆ†äº«å‚æ•°é“¾æ¥ã€‚"""
        try:
            parsed = urlparse(raw_url.strip())
        except Exception:
            return raw_url.strip()

        host = parsed.netloc.lower()
        path = parsed.path.rstrip("/")
        query_items = parse_qsl(parsed.query, keep_blank_values=False)

        if "toutiao.com" in host:
            article_match = re.search(r"/article/(\d+)", path)
            if article_match:
                return f"toutiao:article:{article_match.group(1)}"

            query_map = {k: v for k, v in query_items}
            for key in ("group_id", "article_id", "item_id"):
                if query_map.get(key):
                    return f"toutiao:article:{query_map[key]}"

        filtered_query = []
        for key, value in query_items:
            key_lower = key.lower()
            if key_lower in self._tracking_query_params:
                continue
            if key_lower.startswith("utm_"):
                continue
            filtered_query.append((key, value))

        filtered_query.sort()
        normalized_query = urlencode(filtered_query, doseq=True)
        normalized_path = path or "/"

        return urlunparse((
            parsed.scheme.lower(),
            host,
            normalized_path,
            "",
            normalized_query,
            "",
        ))

    def _deduplicate_urls(self, urls: List[str]) -> List[str]:
        """æŒ‰åŸé¡ºåºå»é‡URLï¼Œé¿å…é‡å¤å¤„ç†ã€‚"""
        seen = set()
        deduplicated_urls: List[str] = []

        for url in urls:
            dedup_key = self._build_dedup_key(url)
            if dedup_key in seen:
                logger.info(f"è·³è¿‡é‡å¤URL: {url}")
                continue
            seen.add(dedup_key)
            deduplicated_urls.append(url)

        return deduplicated_urls

    def _unwrap_image_links_for_x(self, html_content: str, source_url: str) -> str:
        """Xå¸–å­åœºæ™¯ä¸‹ï¼Œå°†ä»…åŒ…è£¹å›¾ç‰‡çš„é“¾æ¥è§£åŒ…ä¸ºçº¯å›¾ç‰‡ï¼Œé¿å…Markdownå‡ºç°å›¾ç‰‡å¤–é“¾åŒ…è£…ã€‚"""
        try:
            domain = (urlparse(source_url).netloc or "").lower()
            if "x.com" not in domain and "twitter.com" not in domain:
                return html_content
        except Exception:
            return html_content

        # å¸¸è§æ ¼å¼ï¼š<a href="..."><img ...></a> æˆ– <a ...><picture>...<img ...></picture></a>
        pattern = re.compile(
            r"<a\b[^>]*>\s*((?:<picture\b[^>]*>.*?</picture>|<img\b[^>]*>))\s*</a>",
            re.IGNORECASE | re.DOTALL,
        )
        return pattern.sub(r"\1", html_content)

    def _strip_markdown_image_wrappers_for_x(self, markdown_content: str, source_url: str) -> str:
        """Xå¸–å­åœºæ™¯ä¸‹ï¼ŒæŠŠ[![...](img)](link)å‹å¹³ä¸º![...](img)ã€‚"""
        try:
            domain = (urlparse(source_url).netloc or "").lower()
            if "x.com" not in domain and "twitter.com" not in domain:
                return markdown_content
        except Exception:
            return markdown_content

        # [![alt](img)](link) -> ![alt](img)
        pattern = re.compile(
            r"\[\s*!\[([^\]]*)\]\(([^)\n]+)\)\s*\]\(([^)\n]+)\)",
            re.IGNORECASE,
        )
        return pattern.sub(r"![\1](\2)", markdown_content)

    def generate_filename(self, title: str, url: str) -> str:
        """
        ç”Ÿæˆæ–‡ä»¶å

        :param title: ç½‘é¡µæ ‡é¢˜
        :param url: ç½‘é¡µURL
        :return: å®‰å…¨çš„æ–‡ä»¶å
        """
        # æ”¹è¿›çš„æ–‡ä»¶åæ¸…ç†é€»è¾‘ï¼Œä¿ç•™æ›´å¤šæœ‰æ„ä¹‰çš„æ ‡ç‚¹ç¬¦å·
        filename = title

        # ç¬¬ä¸€æ­¥ï¼šæ›¿æ¢æ–‡ä»¶ç³»ç»Ÿä¸å…è®¸çš„å­—ç¬¦ä¸ºå®‰å…¨å­—ç¬¦
        # Windows/Linux/macOS æ–‡ä»¶åä¸èƒ½åŒ…å«çš„å­—ç¬¦: \ / : * ? " < > |
        invalid_chars = {
            '\\': 'ï¼¼',  # åæ–œæ  -> å…¨è§’åæ–œæ 
            '/': 'ï¼',   # æ–œæ  -> å…¨è§’æ–œæ 
            ':': 'ï¼š',   # å†’å· -> ä¸­æ–‡å†’å·
            '*': 'ï¼Š',   # æ˜Ÿå· -> å…¨è§’æ˜Ÿå·
            '?': 'ï¼Ÿ',   # é—®å· -> ä¸­æ–‡é—®å·
            '"': 'ï¼‚',   # åŒå¼•å· -> å…¨è§’åŒå¼•å·
            '<': 'ï¼œ',   # å°äºå· -> å…¨è§’å°äºå·
            '>': 'ï¼',   # å¤§äºå· -> å…¨è§’å¤§äºå·
            '|': 'ï½œ'    # ç«–çº¿ -> å…¨è§’ç«–çº¿
        }

        for invalid_char, replacement in invalid_chars.items():
            filename = filename.replace(invalid_char, replacement)

        # ç¬¬äºŒæ­¥ï¼šå¤„ç†è¿ç»­çš„ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        # å°†å¤šä¸ªè¿ç»­ç©ºæ ¼æ›¿æ¢ä¸ºå•ä¸ªç©ºæ ¼
        filename = re.sub(r'\s+', ' ', filename)

        # ç¬¬ä¸‰æ­¥ï¼šæ¸…ç†é¦–å°¾ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
        filename = filename.strip(' .-_')

        # ç¬¬å››æ­¥ï¼šå¦‚æœæ ‡é¢˜ä¸ºç©ºæˆ–å¤ªçŸ­ï¼Œä½¿ç”¨URLçš„åŸŸå
        if not filename or len(filename) < 3:
            from urllib.parse import urlparse
            parsed_url = urlparse(url)
            filename = parsed_url.netloc.replace('.', '-')

        # ç¬¬äº”æ­¥ï¼šé™åˆ¶æ–‡ä»¶åé•¿åº¦ï¼ˆè€ƒè™‘ä¸­æ–‡å­—ç¬¦ï¼‰
        if len(filename) > 80:  # å¢åŠ é•¿åº¦é™åˆ¶ï¼Œå› ä¸ºä¸­æ–‡å­—ç¬¦å ç”¨æ›´å¤šç©ºé—´
            filename = filename[:80]
            # ç¡®ä¿ä¸åœ¨ä¸­æ–‡å­—ç¬¦ä¸­é—´æˆªæ–­
            if len(filename.encode('utf-8')) > len(filename):
                # åŒ…å«ä¸­æ–‡å­—ç¬¦ï¼Œæ›´ä¿å®ˆåœ°æˆªæ–­
                filename = filename[:60]

        # ç¬¬å…­æ­¥ï¼šå†æ¬¡æ¸…ç†å¯èƒ½çš„å°¾éƒ¨å­—ç¬¦
        filename = filename.strip(' .-_')

        # ç¬¬ä¸ƒæ­¥ï¼šæ·»åŠ æ–‡ä»¶æ‰©å±•å
        filename = f"{filename}.md"

        return filename

    def get_unique_filename(self, filename: str) -> str:
        """
        è·å–å”¯ä¸€çš„æ–‡ä»¶åï¼Œå¦‚æœæ–‡ä»¶å·²å­˜åœ¨åˆ™è‡ªåŠ¨æ·»åŠ æ•°å­—åç¼€

        :param filename: åŸå§‹æ–‡ä»¶å
        :return: å”¯ä¸€çš„æ–‡ä»¶å
        """
        output_path = self.output_dir / filename

        # å¦‚æœæ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥è¿”å›åŸæ–‡ä»¶å
        if not output_path.exists():
            return filename

        # æ–‡ä»¶å·²å­˜åœ¨ï¼Œç”Ÿæˆå¸¦æ•°å­—åç¼€çš„æ–‡ä»¶å
        base_name = filename.rsplit('.', 1)[0]  # å»æ‰æ‰©å±•å
        extension = filename.rsplit('.', 1)[-1]  # è·å–æ‰©å±•å

        counter = 1
        while True:
            new_filename = f"{base_name}_{counter}.{extension}"
            new_path = self.output_dir / new_filename

            if not new_path.exists():
                return new_filename

            counter += 1

            # é˜²æ­¢æ— é™å¾ªç¯ï¼Œæœ€å¤šå°è¯•100æ¬¡
            if counter > 100:
                # å¦‚æœè¿˜æ˜¯é‡å¤ï¼Œæ·»åŠ æ—¶é—´æˆ³
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                return f"{base_name}_{timestamp}.{extension}"

    def build_markdown_document(self, page_info: Dict[str, str], url: str, markdown_content: str) -> str:
        """
        æ„å»ºå®Œæ•´çš„Markdownæ–‡æ¡£

        :param page_info: é¡µé¢ä¿¡æ¯
        :param url: åŸå§‹URL
        :param markdown_content: è½¬æ¢åçš„å†…å®¹
        :return: å®Œæ•´çš„Markdownæ–‡æ¡£
        """
        current_time = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

        # æ„å»ºæ–‡æ¡£å¤´éƒ¨
        # header_parts = [f"# {page_info['title']}", ""]
        header_parts = [f"", ""]

        # æ·»åŠ å…ƒæ•°æ®
        metadata = [
            f"**åŸå§‹é“¾æ¥ï¼š** [{url}]({url})",
            f"**ä¿å­˜æ—¶é—´ï¼š** {current_time}",
        ]

        if page_info['description']:
            metadata.append(f"**æè¿°ï¼š** {page_info['description']}")

        if page_info['author']:
            metadata.append(f"**ä½œè€…ï¼š** {page_info['author']}")

        if page_info['publish_time']:
            metadata.append(f"**å‘å¸ƒæ—¶é—´ï¼š** {page_info['publish_time']}")

        header_parts.extend(metadata)
        header_parts.extend(["", "---", ""])

        # ç»„åˆå®Œæ•´æ–‡æ¡£
        full_document = '\n'.join(header_parts) + '\n' + markdown_content

        return full_document

    async def convert_url_to_markdown(self, url: str, output_filename: str = None, download_media: bool = True) -> Optional[Path]:
        """
        å°†URLè½¬æ¢ä¸ºMarkdownæ–‡ä»¶

        :param url: è¦è½¬æ¢çš„URL
        :param output_filename: è¾“å‡ºæ–‡ä»¶åï¼ˆå¯é€‰ï¼‰
        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
        :return: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        self.last_error = {}
        start_time = time.monotonic()

        try:
            # æ£€æŸ¥æ˜¯å¦éœ€è¦ç‰¹æ®Šå¤„ç†
            if self.special_site_handler.can_handle(url):
                logger.info(f"ä½¿ç”¨ç‰¹æ®Šå¤„ç†å™¨å¤„ç†: {url}")
                remaining = self._remaining_seconds(start_time)
                if remaining <= 0:
                    self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "special_handler")
                    return None

                special_result = await self._run_blocking_with_timeout(
                    self.special_site_handler.get_content,
                    url,
                    timeout=remaining
                )

                if special_result:
                    html_content = special_result['html_content']
                    page_info = {
                        'title': special_result['title'],
                        'description': special_result['description'],
                        'author': special_result['author'],
                        'publish_time': special_result['publish_time']
                    }
                else:
                    from urllib.parse import urlparse
                    parsed_url = urlparse(url)
                    domain = parsed_url.netloc.lower()
                    special_error = getattr(self.special_site_handler, "last_error", None)

                    if "toutiao.com" in domain:
                        if special_error:
                            self.last_error = special_error
                        else:
                            self._set_last_error("DYNAMIC_PAGE_UNRESOLVED", "åŠ¨æ€é¡µé¢æœªæˆåŠŸè§£æï¼Œå·²è½¬äººå·¥å¤„ç†", "special_handler")
                        logger.warning("ç‰¹æ®Šå¤„ç†å™¨æœªæˆåŠŸè§£æä»Šæ—¥å¤´æ¡å†…å®¹ï¼Œè½¬äººå·¥å¤„ç†")
                        return None

                    logger.warning("ç‰¹æ®Šå¤„ç†å™¨å¤„ç†å¤±è´¥ï¼Œå›é€€åˆ°æ™®é€šæ–¹å¼")
                    remaining = self._remaining_seconds(start_time)
                    if remaining <= 0:
                        self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "web_fetch")
                        return None

                    html_content = await self._run_blocking_with_timeout(
                        self.web_downloader.fetch_webpage,
                        url,
                        timeout=remaining
                    )
                    if not html_content:
                        if special_error:
                            self.last_error = special_error
                        else:
                            self._set_last_error("FETCH_FAILED", "ç½‘é¡µå†…å®¹æŠ“å–å¤±è´¥", "web_fetch")
                        return None
                    if self.web_downloader._is_js_redirect_page(html_content):
                        special_error = getattr(self.special_site_handler, "last_error", None)
                        if special_error:
                            self.last_error = special_error
                        else:
                            self._set_last_error("DYNAMIC_PAGE_UNRESOLVED", "åŠ¨æ€é¡µé¢æœªæˆåŠŸè§£æï¼Œå·²è½¬äººå·¥å¤„ç†", "web_fetch")
                        return None
                    page_info = self.web_downloader.extract_page_info(html_content)
            else:
                # è·å–ç½‘é¡µå†…å®¹
                remaining = self._remaining_seconds(start_time)
                if remaining <= 0:
                    self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "web_fetch")
                    return None

                html_content = await self._run_blocking_with_timeout(
                    self.web_downloader.fetch_webpage,
                    url,
                    timeout=remaining
                )
                if not html_content:
                    self._set_last_error("FETCH_FAILED", "ç½‘é¡µå†…å®¹æŠ“å–å¤±è´¥", "web_fetch")
                    return None

                # æ£€æŸ¥æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢
                if self.web_downloader._is_js_redirect_page(html_content):
                    logger.warning(f"æ£€æµ‹åˆ°JavaScripté‡å®šå‘é¡µé¢: {url}")
                    
                    # å¦‚æœæ£€æµ‹åˆ°é‡å®šå‘é¡µé¢ï¼Œä¸”URLæ˜¯ä»Šæ—¥å¤´æ¡ï¼Œå°è¯•ä½¿ç”¨ç‰¹æ®Šå¤„ç†å™¨
                    from urllib.parse import urlparse
                    parsed_url = urlparse(url)
                    domain = parsed_url.netloc.lower()
                    
                    if 'toutiao.com' in domain and self.special_site_handler.can_handle(url):
                        logger.info("æ£€æµ‹åˆ°ä»Šæ—¥å¤´æ¡é“¾æ¥ä¸”ä¸ºJavaScripté‡å®šå‘é¡µé¢ï¼Œå°è¯•ä½¿ç”¨Playwrightå¤„ç†...")
                        remaining = self._remaining_seconds(start_time)
                        if remaining <= 0:
                            self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "special_handler")
                            return None

                        special_result = await self._run_blocking_with_timeout(
                            self.special_site_handler.get_content,
                            url,
                            timeout=remaining
                        )
                        
                        if special_result:
                            logger.info("âœ… ä½¿ç”¨PlaywrightæˆåŠŸè·å–å†…å®¹")
                            html_content = special_result['html_content']
                            page_info = {
                                'title': special_result['title'],
                                'description': special_result['description'],
                                'author': special_result['author'],
                                'publish_time': special_result['publish_time']
                            }
                        else:
                            logger.warning("Playwrightå¤„ç†å¤±è´¥ï¼Œè½¬äººå·¥å¤„ç†")
                            special_error = getattr(self.special_site_handler, "last_error", None)
                            if special_error:
                                self.last_error = special_error
                            else:
                                self._set_last_error("DYNAMIC_PAGE_UNRESOLVED", "åŠ¨æ€é¡µé¢æœªæˆåŠŸè§£æï¼Œå·²è½¬äººå·¥å¤„ç†", "special_handler")
                            logger.info(self.special_site_handler.get_installation_guide())
                            return None
                    else:
                        # ä¸æ˜¯ä»Šæ—¥å¤´æ¡æˆ–å…¶ä»–ç‰¹æ®Šç½‘ç«™ï¼Œåªæç¤º
                        page_info = self.web_downloader.extract_page_info(html_content)
                        logger.info(self.special_site_handler.get_installation_guide())
                else:
                    # æ­£å¸¸é¡µé¢ï¼Œç›´æ¥æå–ä¿¡æ¯
                    page_info = self.web_downloader.extract_page_info(html_content)

            # logger.info(f"é¡µé¢æ ‡é¢˜: {page_info['title']}")

            # ä¸‹è½½åª’ä½“æ–‡ä»¶
            media_mapping = {}
            if download_media:
                # logger.info("å¼€å§‹ä¸‹è½½åª’ä½“æ–‡ä»¶...")
                media_urls = self.html_converter.extract_media_urls(html_content, url)
                # logger.info(f"å‘ç° {len(media_urls)} ä¸ªåª’ä½“æ–‡ä»¶")

                if media_urls:
                    remaining = self._remaining_seconds(start_time)
                    if remaining <= 0:
                        self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "media_download")
                        return None
                    media_mapping = await asyncio.wait_for(
                        self.media_downloader.download_media_batch(media_urls, url),
                        timeout=remaining
                    )
                    # logger.info(f"æˆåŠŸä¸‹è½½ {len(media_mapping)} ä¸ªåª’ä½“æ–‡ä»¶")
                # else:
                #     logger.warning("æœªå‘ç°ä»»ä½•åª’ä½“æ–‡ä»¶URL")

            # æ¸…ç†HTMLå†…å®¹
            clean_html = self.html_converter.clean_html_for_conversion(html_content)
            clean_html = self._unwrap_image_links_for_x(clean_html, url)

            # è½¬æ¢ä¸ºMarkdown
            markdown_content = self.html_converter.convert_html_to_markdown(clean_html)
            markdown_content = self._strip_markdown_image_wrappers_for_x(markdown_content, url)

            # æ›´æ–°åª’ä½“é“¾æ¥
            if media_mapping:
                markdown_content = self.html_converter.update_media_links(markdown_content, media_mapping)

            # ç”Ÿæˆæ–‡ä»¶å
            if not output_filename:
                output_filename = self.generate_filename(page_info['title'], url)

            # ç¡®ä¿æ–‡ä»¶åå”¯ä¸€ï¼Œé¿å…è¦†ç›–
            output_filename = self.get_unique_filename(output_filename)
            output_path = self.output_dir / output_filename

            # æ„å»ºå®Œæ•´æ–‡æ¡£
            full_markdown = self.build_markdown_document(page_info, url, markdown_content)

            # ä¿å­˜æ–‡ä»¶
            output_path.write_text(full_markdown, encoding='utf-8')
            logger.info(f"Markdownæ–‡ä»¶å·²ä¿å­˜: {output_path}")
            self.last_error = {}
            return output_path

        except asyncio.TimeoutError:
            self._set_last_error("TIMEOUT_TOTAL", f"è¶…æ—¶ï¼šå•URLå¤„ç†è¶…è¿‡{self.url_timeout_seconds}ç§’", "timeout_guard")
            logger.error(f"è½¬æ¢è¶…æ—¶: {url}")
            return None
        except Exception as e:
            if not self.last_error:
                self._set_last_error("CONVERT_ERROR", str(e), "convert")
            logger.error(f"è½¬æ¢è¿‡ç¨‹å‡ºé”™: {e}")
            return None

    async def convert_urls_from_clipboard(self, download_media: bool = True) -> List[Path]:
        """
        ä»å‰ªè´´æ¿è¯»å–URLå¹¶æ‰¹é‡è½¬æ¢ä¸ºMarkdownæ–‡ä»¶

        :param download_media: æ˜¯å¦ä¸‹è½½åª’ä½“æ–‡ä»¶
        :return: æˆåŠŸè½¬æ¢çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨
        """
        urls, collected_notes = self.clipboard_manager.read_urls_from_clipboard()
        urls = self._deduplicate_urls(urls)

        # logger.info(f"å¼€å§‹æ‰¹é‡è½¬æ¢ {len(urls)} ä¸ªURL...")
        successful_files = []
        failed_items: List[Dict[str, str]] = []
        if urls:
            for i, url in enumerate(urls, 1):
                try:
                    # æ£€æŸ¥æ˜¯å¦æ˜¯é£ä¹¦é“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™æ˜¾ç¤ºç‰¹æ®Šæç¤º
                    if url.startswith("https://waytoagi.feishu.cn/"):
                        logger.info(f"\n[{i}/{len(urls)}] æ­£åœ¨å¤„ç†é£ä¹¦é“¾æ¥: {url}")
                        logger.info("  ğŸ” æ£€æµ‹é£ä¹¦é“¾æ¥ï¼Œå°†è‡ªåŠ¨æå–å¾®ä¿¡åŸæ–‡é“¾æ¥...")
                    else:
                        logger.info(f"\n[{i}/{len(urls)}] æ­£åœ¨è½¬æ¢: {url}")

                    # è½¬æ¢URLä¸ºMarkdownï¼Œä¸æŒ‡å®šæ–‡ä»¶åï¼Œè®©ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ
                    result_path = await self.convert_url_to_markdown(
                        url=url,
                        output_filename=None,  # ä½¿ç”¨ç½‘é¡µæ ‡é¢˜è‡ªåŠ¨ç”Ÿæˆæ–‡ä»¶å
                        download_media=download_media
                    )

                    if result_path:
                        successful_files.append(result_path)
                        file_size = result_path.stat().st_size
                        # logger.info(f"    âœ“ è½¬æ¢æˆåŠŸ! æ–‡ä»¶: {result_path.name} ({file_size:,} å­—èŠ‚)")
                    else:
                        logger.error(f"    âœ— è½¬æ¢å¤±è´¥: {url}")
                        last_error = self.last_error or {}
                        failed_items.append({
                            "url": url,
                            "message": last_error.get("message", "è½¬æ¢å¤±è´¥ï¼Œæœªè¿”å›å…·ä½“é”™è¯¯"),
                            "stage": last_error.get("stage", ""),
                            "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                        })

                except Exception as e:
                    logger.error(f"    âœ— è½¬æ¢å‡ºé”™: {url}, é”™è¯¯: {e}")
                    failed_items.append({
                        "url": url,
                        "message": str(e),
                        "stage": "batch_loop",
                        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    })

            logger.info(f"\næ‰¹é‡è½¬æ¢å®Œæˆ! æˆåŠŸ: {len(successful_files)}/{len(urls)}")
        else:
            logger.warning("æ²¡æœ‰ä»å‰ªè´´æ¿æ‰¾åˆ°æœ‰æ•ˆçš„URL")

        self._save_collected_notes(collected_notes, failed_items)
        return successful_files

    def is_clipboard_available(self) -> bool:
        """æ£€æŸ¥å‰ªè´´æ¿åŠŸèƒ½æ˜¯å¦å¯ç”¨"""
        return self.clipboard_manager.is_available()
