"""
ç½‘é¡µä¸‹è½½å™¨æ¨¡å—
è´Ÿè´£è·å–ç½‘é¡µå†…å®¹å¹¶æå–é¡µé¢ä¿¡æ¯
"""

import requests
import logging
import re
import html
import gzip
import zlib
from typing import Optional, Dict
from urllib.parse import urlparse

try:
    from bs4 import BeautifulSoup
    HAS_BS4 = True
except ImportError:
    HAS_BS4 = False

try:
    import brotli
    HAS_BROTLI = True
except ImportError:
    HAS_BROTLI = False

try:
    import chardet
    HAS_CHARDET = True
except ImportError:
    HAS_CHARDET = False

logger = logging.getLogger(__name__)


class WebDownloader:
    """ç½‘é¡µä¸‹è½½å™¨"""

    def __init__(self):
        """åˆå§‹åŒ–ç½‘é¡µä¸‹è½½å™¨"""
        # è¯·æ±‚å¤´è®¾ç½®
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Accept-Encoding': 'gzip, deflate, br',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
            'Sec-Fetch-Dest': 'document',
            'Sec-Fetch-Mode': 'navigate',
            'Sec-Fetch-Site': 'none',
            'Cache-Control': 'max-age=0'
        }

        # é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„ç‰¹æ®Šè¯·æ±‚å¤´
        self.site_specific_headers = {
            'toutiao.com': {
                'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
                'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8',
                'Accept-Encoding': 'gzip, deflate, br',
                'Sec-Ch-Ua': '"Not_A Brand";v="8", "Chromium";v="120", "Google Chrome";v="120"',
                'Sec-Ch-Ua-Mobile': '?0',
                'Sec-Ch-Ua-Platform': '"Windows"',
                'Sec-Fetch-Dest': 'document',
                'Sec-Fetch-Mode': 'navigate',
                'Sec-Fetch-Site': 'none',
                'Sec-Fetch-User': '?1',
                'Upgrade-Insecure-Requests': '1'
            }
        }

    def _get_site_specific_headers(self, url: str) -> Dict[str, str]:
        """è·å–é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„è¯·æ±‚å¤´"""
        parsed_url = urlparse(url)
        domain = parsed_url.netloc.lower()

        # æ£€æŸ¥æ˜¯å¦æœ‰é’ˆå¯¹è¯¥åŸŸåçš„ç‰¹æ®Šè¯·æ±‚å¤´
        for site_pattern, headers in self.site_specific_headers.items():
            if site_pattern in domain:
                return {**self.headers, **headers}

        return self.headers

    def _decompress_content(self, content: bytes, encoding: str) -> bytes:
        """è§£å‹ç¼©å†…å®¹"""
        try:
            if encoding == 'gzip':
                return gzip.decompress(content)
            elif encoding == 'deflate':
                return zlib.decompress(content)
            elif encoding == 'br' and HAS_BROTLI:
                return brotli.decompress(content)
            else:
                return content
        except Exception as e:
            logger.warning(f"è§£å‹ç¼©å¤±è´¥ ({encoding}): {e}")
            return content

    def _detect_encoding(self, content: bytes, response_encoding: str = None) -> str:
        """æ£€æµ‹å†…å®¹ç¼–ç """
        # é¦–å…ˆå°è¯•å“åº”å¤´ä¸­çš„ç¼–ç 
        if response_encoding and response_encoding.lower() not in ['iso-8859-1', 'ascii']:
            return response_encoding

        # ä½¿ç”¨chardetæ£€æµ‹
        if HAS_CHARDET:
            detected = chardet.detect(content)
            if detected['encoding'] and detected['confidence'] > 0.7:
                return detected['encoding']

        # å°è¯•ä»HTML metaæ ‡ç­¾ä¸­æå–ç¼–ç 
        try:
            content_str = content.decode('utf-8', errors='ignore')
            charset_match = re.search(r'<meta[^>]*charset[="\s]*([^">\s]+)', content_str, re.IGNORECASE)
            if charset_match:
                return charset_match.group(1)
        except:
            pass

        # æŒ‰ä¼˜å…ˆçº§å°è¯•å¸¸è§ç¼–ç 
        encodings = ['utf-8', 'gbk', 'gb2312', 'gb18030', 'big5', 'latin-1']
        for encoding in encodings:
            try:
                content.decode(encoding)
                return encoding
            except UnicodeDecodeError:
                continue

        return 'utf-8'  # é»˜è®¤ä½¿ç”¨UTF-8

    def _extract_wechat_original_link(self, content: str) -> Optional[str]:
        """
        ä»é£ä¹¦é¡µé¢å†…å®¹ä¸­æå–å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥

        :param content: ç½‘é¡µHTMLå†…å®¹
        :return: å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥ï¼Œå¦‚æœæ²¡æœ‰æ‰¾åˆ°åˆ™è¿”å›None
        """
        try:
            # åŒ¹é…å„ç§å¯èƒ½çš„åŸæ–‡é“¾æ¥æ ¼å¼ï¼Œä½¿ç”¨æ›´ç²¾ç¡®çš„æ­£åˆ™è¡¨è¾¾å¼
            patterns = [
                r"""åŸæ–‡é“¾æ¥[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)""",
                r"""åŸæ–‡é“¾æ¥\s*[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)""",
                r"""åŸæ–‡é“¾æ¥\s+[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+)""",
                r"""åŸæ–‡é“¾æ¥[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"'\s<>]+)""",
                r"""åŸæ–‡é“¾æ¥\s*[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"'\s<>]+)""",
                r"""åŸæ–‡é“¾æ¥\s+[ï¼š:]\s*(https://mp\.weixin\.qq\.com/s\?[^"'\s<>]+)""",
            ]

            wechat_patterns = [
                r"https://mp\.weixin\.qq\.com/s/[A-Za-z0-9_-]+",  # çŸ­é“¾æ¥æ ¼å¼
                r"""https://mp\.weixin\.qq\.com/s\?[^"'\s<>]+""",    # é•¿é“¾æ¥æ ¼å¼
            ]

            lines = content.split('\n')[:10]

            def normalize_link(raw_link: str) -> str:
                """æ¸…æ´—HTMLä¸­çš„åŸæ–‡é“¾æ¥ï¼Œç§»é™¤è½¬ä¹‰ç¬¦å’Œå†—ä½™ç‰‡æ®µ"""
                cleaned = html.unescape(raw_link.strip())
                cleaned = cleaned.replace('\u200b', '').replace('\u00a0', '')
                if '#wechat_redirect' in cleaned:
                    cleaned = cleaned.split('#wechat_redirect', 1)[0]
                cleaned = cleaned.rstrip('?')
                return cleaned

            def add_candidate(raw_link: str, candidates: list, seen: set):
                cleaned = normalize_link(raw_link)
                if not cleaned:
                    return
                if '...' in cleaned or cleaned.endswith('...'):
                    return
                if cleaned not in seen:
                    seen.add(cleaned)
                    candidates.append(cleaned)

            def select_best_link(candidates: list) -> Optional[str]:
                if not candidates:
                    return None

                def is_short_form(link: str) -> bool:
                    return link.startswith('https://mp.weixin.qq.com/s/') and '?' not in link

                def is_long_enough(link: str) -> bool:
                    return len(link) > 40

                short_links = [link for link in candidates if is_short_form(link) and is_long_enough(link)]
                if short_links:
                    return short_links[0]

                query_links = [link for link in candidates if link.startswith('https://mp.weixin.qq.com/s?') and is_long_enough(link)]
                if query_links:
                    return query_links[0]

                long_links = [link for link in candidates if is_long_enough(link)]
                if long_links:
                    return long_links[0]

                return candidates[0]

            candidates = []
            seen_links = set()

            # ä¼˜å…ˆåœ¨å‰10è¡Œä¸­å°è¯•æ”¶é›†å€™é€‰é“¾æ¥
            for line in lines:
                for pattern in patterns:
                    match = re.search(pattern, line)
                    if match:
                        add_candidate(match.group(1), candidates, seen_links)

            # å¦‚æœå‰10è¡Œæœªæ‰¾åˆ°è¶³å¤Ÿä¿¡æ¯ï¼Œåˆ™æ‰«ææ•´ä¸ªé¡µé¢
            for pattern in wechat_patterns:
                matches = re.findall(pattern, content)
                for link in matches:
                    add_candidate(link, candidates, seen_links)

            if candidates:
                original_url = select_best_link(candidates)
                if original_url:
                    logger.info(f"åœ¨é£ä¹¦é¡µé¢ä¸­æ‰¾åˆ°å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥: {original_url}")
                    return original_url

            return None

        except Exception as e:
            logger.error(f"æå–å¾®ä¿¡åŸæ–‡é“¾æ¥æ—¶å‡ºé”™: {e}")
            return None

    def fetch_webpage(self, url: str) -> Optional[str]:
        """
        è·å–ç½‘é¡µå†…å®¹

        :param url: ç½‘é¡µURL
        :return: ç½‘é¡µHTMLå†…å®¹ï¼Œå¤±è´¥æ—¶è¿”å›None
        """
        try:
            # æ£€æŸ¥æ˜¯å¦æ˜¯é£ä¹¦é“¾æ¥ï¼Œå¦‚æœæ˜¯åˆ™å…ˆå°è¯•æå–åŸæ–‡é“¾æ¥
            if url.startswith("https://waytoagi.feishu.cn/"):
                logger.info(f"æ£€æµ‹åˆ°é£ä¹¦é“¾æ¥ï¼Œå°è¯•æå–å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥: {url}")

                # å…ˆè·å–é£ä¹¦é¡µé¢å†…å®¹
                headers = self._get_site_specific_headers(url)
                response = requests.get(url, headers=headers, timeout=30)
                response.raise_for_status()

                # è·å–åŸå§‹å†…å®¹å¹¶è§£ç 
                raw_content = response.content
                content_encoding = response.headers.get('Content-Encoding', '').lower()
                if content_encoding:
                    raw_content = self._decompress_content(raw_content, content_encoding)

                detected_encoding = self._detect_encoding(raw_content, response.encoding)
                try:
                    feishu_content = raw_content.decode(detected_encoding)
                except UnicodeDecodeError:
                    try:
                        feishu_content = raw_content.decode('utf-8', errors='replace')
                    except:
                        feishu_content = raw_content.decode('latin-1')

                # å°è¯•æå–å¾®ä¿¡åŸæ–‡é“¾æ¥
                original_url = self._extract_wechat_original_link(feishu_content)

                if original_url:
                    logger.info(f"  âœ… æ‰¾åˆ°å¾®ä¿¡åŸæ–‡é“¾æ¥: {original_url}")
                    logger.info(f"  ğŸ”„ é‡å®šå‘åˆ°åŸæ–‡é“¾æ¥è·å–å†…å®¹...")
                    # é€’å½’è°ƒç”¨ï¼Œä½¿ç”¨åŸæ–‡é“¾æ¥è·å–å†…å®¹
                    return self.fetch_webpage(original_url)
                else:
                    logger.info("  âŒ æœªæ‰¾åˆ°å¾®ä¿¡å…¬ä¼—å·åŸæ–‡é“¾æ¥ï¼Œä½¿ç”¨é£ä¹¦é¡µé¢å†…å®¹")
                    return feishu_content

            # è·å–é’ˆå¯¹ç‰¹å®šç½‘ç«™çš„è¯·æ±‚å¤´
            headers = self._get_site_specific_headers(url)

            # logger.info(f"æ­£åœ¨è·å–ç½‘é¡µå†…å®¹: {url}")
            response = requests.get(url, headers=headers, timeout=30)
            response.raise_for_status()

            # è·å–åŸå§‹å†…å®¹
            raw_content = response.content

            # å¤„ç†å‹ç¼©å†…å®¹
            content_encoding = response.headers.get('Content-Encoding', '').lower()
            if content_encoding:
                raw_content = self._decompress_content(raw_content, content_encoding)

            # æ£€æµ‹ç¼–ç 
            detected_encoding = self._detect_encoding(raw_content, response.encoding)

            # è§£ç å†…å®¹
            try:
                content = raw_content.decode(detected_encoding)
            except UnicodeDecodeError:
                # å¦‚æœæ£€æµ‹çš„ç¼–ç å¤±è´¥ï¼Œå°è¯•UTF-8
                try:
                    content = raw_content.decode('utf-8', errors='replace')
                except:
                    content = raw_content.decode('latin-1')

            # æ£€æŸ¥æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢ï¼ˆå¦‚ä»Šæ—¥å¤´æ¡ï¼‰
            if self._is_js_redirect_page(content):
                logger.warning(f"æ£€æµ‹åˆ°JavaScripté‡å®šå‘é¡µé¢: {url}")
                logger.info("æç¤º: å¦‚æœå®‰è£…äº† Playwrightï¼Œç¨‹åºä¼šè‡ªåŠ¨ä½¿ç”¨å®ƒå¤„ç†æ­¤ç±»é¡µé¢")
                # æ³¨æ„ï¼šå®é™…çš„ç‰¹æ®Šå¤„ç†é€»è¾‘åœ¨url_to_markdown.pyä¸­é€šè¿‡special_site_handlerå®ç°

            # logger.info(f"æˆåŠŸè·å–ç½‘é¡µå†…å®¹ï¼Œå¤§å°: {len(content)} å­—ç¬¦")
            return content

        except requests.exceptions.RequestException as e:
            logger.error(f"è·å–ç½‘é¡µå¤±è´¥: {e}")
            return None

    def _is_js_redirect_page(self, content: str) -> bool:
        """
        æ£€æµ‹æ˜¯å¦æ˜¯JavaScripté‡å®šå‘é¡µé¢
        
        ä»Šæ—¥å¤´æ¡ç­‰ç½‘ç«™ä¼šè¿”å›ä¸€ä¸ªåŒ…å«å¤§é‡JavaScriptä»£ç çš„é¡µé¢ï¼Œ
        è€Œä¸æ˜¯å®é™…çš„HTMLå†…å®¹ã€‚è¿™ä¸ªæ–¹æ³•ç”¨äºæ£€æµ‹è¿™ç§æƒ…å†µã€‚
        """
        # æ£€æŸ¥å†…å®¹é•¿åº¦å’Œç‰¹å¾
        if len(content) < 1000:
            return False

        # æ£€æŸ¥æ˜¯å¦ä¸»è¦åŒ…å«JavaScriptä»£ç è€Œç¼ºå°‘å®é™…å†…å®¹
        js_indicators = [
            'window._$jsvmprt',
            'var glb;',
            'function(b,e,f)',
            'typeof window?global:window',
            '_$jsvmprt=function',
            '<script>var glb'
        ]

        js_count = sum(1 for indicator in js_indicators if indicator in content)

        # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘å®é™…çš„HTMLå†…å®¹ç»“æ„
        html_indicators = [
            '<article',
            '<main',
            '<div class="content',
            '<div class="article',
            '<p>',
            '<body><script>',  # å¦‚æœbodyæ ‡ç­¾åç›´æ¥æ˜¯scriptï¼Œå¯èƒ½æ˜¯é‡å®šå‘é¡µé¢
        ]
        html_count = sum(1 for indicator in html_indicators if indicator in content)
        
        # æ£€æŸ¥bodyæ ‡ç­¾æ˜¯å¦ä¸ºç©ºæˆ–åªåŒ…å«script
        import re
        body_match = re.search(r'<body[^>]*>(.*?)</body>', content, re.IGNORECASE | re.DOTALL)
        if body_match:
            body_content = body_match.group(1).strip()
            # å¦‚æœbodyå†…å®¹ä¸»è¦æ˜¯scriptæ ‡ç­¾ï¼Œå¯èƒ½æ˜¯é‡å®šå‘é¡µé¢
            if body_content and len(body_content) > 0:
                script_ratio = len(re.findall(r'<script', body_content, re.IGNORECASE)) / max(len(re.findall(r'<[^>]+>', body_content)), 1)
                if script_ratio > 0.7:  # å¦‚æœè¶…è¿‡70%çš„æ ‡ç­¾æ˜¯scriptï¼Œå¯èƒ½æ˜¯é‡å®šå‘é¡µé¢
                    return True

        # å¦‚æœæ£€æµ‹åˆ°å¤šä¸ªJavaScriptæŒ‡ç¤ºå™¨ä¸”ç¼ºå°‘HTMLå†…å®¹ï¼Œåˆ¤å®šä¸ºé‡å®šå‘é¡µé¢
        return js_count >= 2 and html_count == 0

    def extract_page_info(self, html_content: str) -> Dict[str, str]:
        """
        æå–é¡µé¢åŸºæœ¬ä¿¡æ¯

        :param html_content: HTMLå†…å®¹
        :return: é¡µé¢ä¿¡æ¯å­—å…¸
        """
        if not HAS_BS4:
            logger.warning("BeautifulSoupæœªå®‰è£…ï¼Œä½¿ç”¨ç®€å•è§£æ")
            return self._extract_page_info_simple(html_content)

        soup = BeautifulSoup(html_content, 'html.parser')

        # æå–æ ‡é¢˜
        title = "æœªçŸ¥æ ‡é¢˜"
        title_tag = soup.find('title')
        if title_tag and title_tag.string:
            title = title_tag.string.strip()
        else:
            h1_tag = soup.find('h1')
            if h1_tag:
                title = h1_tag.get_text().strip()

        # æ¸…ç†æ ‡é¢˜
        title = re.sub(r'\s+', ' ', title)

        # æå–æè¿°
        description = ""
        desc_tag = soup.find('meta', attrs={'name': 'description'})
        if desc_tag and desc_tag.get('content'):
            description = desc_tag.get('content').strip()
        else:
            og_desc_tag = soup.find('meta', attrs={'property': 'og:description'})
            if og_desc_tag and og_desc_tag.get('content'):
                description = og_desc_tag.get('content').strip()

        # æå–ä½œè€…
        author = ""
        author_tag = soup.find('meta', attrs={'name': 'author'})
        if author_tag and author_tag.get('content'):
            author = author_tag.get('content').strip()

        # æå–å‘å¸ƒæ—¶é—´
        publish_time = ""
        time_selectors = [
            'meta[property="article:published_time"]',
            'meta[name="publish_date"]',
            'time[datetime]',
            '.publish-time',
            '.date'
        ]

        for selector in time_selectors:
            element = soup.select_one(selector)
            if element:
                if element.get('content'):
                    publish_time = element.get('content')
                elif element.get('datetime'):
                    publish_time = element.get('datetime')
                else:
                    publish_time = element.get_text().strip()
                break

        return {
            'title': title,
            'description': description,
            'author': author,
            'publish_time': publish_time
        }

    def _extract_page_info_simple(self, html_content: str) -> Dict[str, str]:
        """ç®€å•çš„é¡µé¢ä¿¡æ¯æå–ï¼ˆä¸ä½¿ç”¨BeautifulSoupï¼‰"""
        # æå–æ ‡é¢˜
        title_match = re.search(r'<title[^>]*>(.*?)</title>', html_content, re.IGNORECASE | re.DOTALL)
        title = title_match.group(1).strip() if title_match else "æœªçŸ¥æ ‡é¢˜"
        title = html.unescape(re.sub(r'\s+', ' ', title))

        # æå–æè¿°
        desc_match = re.search(r'<meta[^>]*name=["\']description["\'][^>]*content=["\']([^"\']*)["\']', html_content, re.IGNORECASE)
        description = html.unescape(desc_match.group(1).strip()) if desc_match else ""

        return {
            'title': title,
            'description': description,
            'author': "",
            'publish_time': ""
        }
